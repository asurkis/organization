Лектор: Алексей Александрович Шпильман
Практик: Олег Свидченко

# Конспект
**Обучение с учителем (supervised learning)** — на каждый вход из обучающей выборки известен правильный ответ.

**kNN** (k Nearest Neighbors) — ответ для точки определяется голосованием ответов её $k$ ближайших соседей из обучающей выборки. При этом для работы метода достаточно существования функции расстояния.

$$
h(\text{x}, D, k) = \arg\max_{y \in Y} \sum_{\text{x}_i \in D} \left[ y_i = y \right] w(\text{x}_i, x, k)
$$

$$
\begin{aligned}
w(\text{x}_i, x, k) & = 1 \text{, если $\text{x}_i$ --- один из $k$ ближайших соседей x} \\
\text{или} \\
w(\text{x}_i, x, k) & = 1 \text{, если $\rho(\text{x}_i, x) \leq \text{радиус $k$-го соседа}$ } \\
\end{aligned}
$$

Метрика ошибки kNN: **leave-one-out error** — для каждой точки определяется её ответ, если её убрать из обучающей выборки, после этого считается точность (accuracy) классификации (т.к. функция ошибки, то инвертированная точность). Выбирается такое $k$, при котором достигается наилучшая точность (т.е. наименьшая ошибка).

$$
\text{LOO}(k, D) = \frac{\sum_{\text{x}_i \in D}\left[ h(\text{x}_i, D \backslash \text{x}_i, k) \neq y_i \right]}{|D|}
$$

**Train-test split** — выделяем часть выборки для *проверки*, насколько хорошо мы обучились. Тестовая выборка исключается из обучающей.

**Cross-validation** — проверка метрик на разных разбиениях на обучающую и тестовую выборки.

**WkNN** (Weighted k Nearest Neighbors) — вводится **ядро**, например

$$
\begin{aligned}
w_i & = \left[ \frac{r - \rho(\text{x}, \text{x}_i)}{r} \right]_+ \text{ --- треугольное ядро} \\
w_i & = q^{-\rho(\text{x}, \text{x}_i)} \text{ --- гауссово ядро} \\
\end{aligned}
$$

То есть убывающая неотрицательная функция от расстояния, в идеале — обращающаяся в 0 при достижении $r$. Ядро используется для вычисления весов в голосовании.

**Точность классификации:**

|             | Правильный ответ — True     | Правильный ответ — False |                                       |
|-------------|-----------------------------|--------------------------|---------------------------------------|
| Ответ True  | True Positive               | False Positive           | Precision = Positive Predictive Value |
| Ответ False | False Negative              | True Negative            |                                       |
|             | Recall = True Positive Rate | False Positive Rate      | Accuracy                              |
$$
\begin{aligned}
\text{Precision} & = \frac{TP}{TP + FP} \\
\text{Recall} & = \frac{TP}{TP + FN} \\
\end{aligned}
$$

Precision и Recall считаются отдельно для каждого класса, Accuracy (доля попаданий) — можно для всех вместе:

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**KDTree** (k-dimensional Tree) — для быстрого поиска ближайших соседей можно разбить пространство по медиане какой-нибудь координаты. Очевидно, что если расстояние от точки до "многомерного прямоугольника" больше, чем до найденного $k$-го ближайшего соседа, то улучшить результат какой-то точкой в этом прямоугольнике уже невозможно. Такая эвристика позволяет многократно сократить время поиска ближайших соседей: сначала находим прямоугольник, в который попадает точка, ищем соседей в нём, потом поднимаемся по дереву и ищем в нерассмотренных поддеревьях, сразу пропуская всё поддерево целиком, если расстояние слишком большое. Разбиение ветки стоит делать по медиане координаты (для более сбалансированного дерева), и пока *наименьшее* количество точек в поддереве больше заданного порога.

# Экзамен
## Билет 1
### Регрессия, борьба с выбросами. RANSAC. Theil-Sen. Huber.
### Кластеризация. kMeans, kMeans++, MeanShift, DBSCAN.

## Билет 2
### Смещение и дисперсия, понятие средней гипотезы.
### Ансамбли. Soft and Hard Voting. Bagging. Случайный лес. AdaBoost.

## Билет 3
### Линейная регрессия. LASSO, LARS. CART. SVR.
### Деревья решений. Информационный выигрыш. Ошибка классификации, энтропия, критерий Джини.

## Билет 4
### Глобальный поиск. Случайный поиск. Grid search. Случайное блуждание. Байесовская оптимизация. Кросс-энтропийный поиск.
### Линейная регрессия. Полиномиальная регрессия. Гребневая регрессия.

## Билет 5
### Градиентный бустинг решающих деревьев.
### Кластеризация. Agglomerative Clustering. Метрики кластеризации.

## Билет 6
### Оценка классификации. Эффективность по Парето. Precision-Recall и ROC кривые. AUC.
### Векторное представление слов. Word2Vec. Transformer.

## Билет 7
### Локальный поиск. Hill Climb и его разновидности. Отжиг. Генетический алгоритм.
### Метод опорных векторов. Прямая и двойственная задача. Решение двойственной задачи. Типы опорных векторов. Ядра.

## Билет 8
### Гипотезы и дихотомии. Функция роста. Точка поломки. Доказательство полиномиальности функции роста в присутствии точки поломки.
### Деревья решений. Прунинг. Небрежные решающие деревья. Нечеткие решающие деревья.

## Билет 9
### Байесовский классификатор. Оценка признаков (Gaussian, Bernoulli, Multinomial). EM алгоритм.
### Нейронные сети. Перцептрон Розенблатта. Обратное распространение градиента. Функции активации. Softmax.

## Билет 10
### Сверточные нейронные сети. VGG. ResNet. Трансферное обучение.
### Метрический классификаторы. kNN. WkNN. Отбор эталонов. DROP5. KDtree.
