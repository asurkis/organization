Лектор: Алексей Александрович Шпильман
Практик: Олег Свидченко

# Разная теория
## Общие понятия
**Обучение с учителем (supervised learning)** — на каждый вход из обучающей выборки известен правильный ответ. Задача: аппроксимировать целевую функцию.

**Обучение без учителя (unsupervised learning)** — известны входные данные, но не выходные. Цели: извлечение информации, выделение зависимостей, сжатие данных.

**Train-test split** — выделяем часть выборки для _проверки_, насколько хорошо обучилась модель. Тестовая выборка исключается из обучающей.

**Cross-validation** — проверка метрик на разных разбиениях на обучающую и тестовую выборки.

**One Hot Encoding** — каждый категориальный признак кодируется как вектор из нулей и одной единицы (количество измерений — количество возможных классов).

**Утечка данных** — протекание тестовой выборки в обучающую. Опасно тем, что метрика модели будет оцениваться неправильно, потому что может быть переобучение на _тестовых_ данных.

**R<sup>2</sup>-score** — "доля объяснённой дисперсии":

$$
R^2 = 1 - \frac{\sum (h(\vec{x}_i) - y_i)^2}{\sum (\overline{y} - y_i)^2} < 1
$$

## Scalers
Масштабирование нужно для того, чтобы модель учитывала все фичи, а не только большие по модулю. Также может быть полезно центрировать фичу вокруг нуля.

MinMax scaler:

$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)} \cdot (\text{max}' - \text{min}') + \text{min}'
$$

MaxAbs scaler:

$$
x' = \frac{x}{\max(|x|)}
$$

Standard scaler:

$$
x' = \frac{x - \text{mean}(x)}{\text{std}(x)} = \frac{x - E(x)}{\sigma(x)}
$$

Robust scaler:

$$
x' = \frac{x - \text{median}(x)}{\text{quartile}_3(x) - \text{quartile}_1(x)}
$$


## Другое
**Перцептрон:** класс — знак в линейной регрессии. Алгоритм поиска весов:
1. Начать со случайными $W$
2. Для каждого $\vec{x}_i \mid h(\vec{x}_i) \neq y_i$ выполнить $W \gets W + y_i \vec{x}_i$
Алгоритм выше сходится, если множества линейно разделимы.

Если неразделимы, то можно попытаться добавить признаков.

# Экзамен
## Билет 1
### Регрессия, борьба с выбросами. RANSAC. Theil-Sen. Huber.
**Theil-Sen** — обучить несколько моделей на подмножествах обучающей выборки, в качестве ответа брать медиану ответов нескольких моделей.

**RANSAC (RANdom SAmple Consensus)** — обучить модели на подмножествах обучающей выборки, выбрать лучшую по количеству попаданий (полоса), обучить новую модель на этих попаданиях.

**Huber regressor** — квадратичная ошибка для попаданий, линейная — для выбросов. Так выбросы влияют на результат значительно меньше.

$$
\begin{gather*}
\min_{w, \sigma} \sum_{i=1}^N \left( \sigma + H_\varepsilon \left( \frac{\vec{x}_i W - y_i}{\sigma} \right) \sigma \right) + \alpha ||W||_2^2 \\
H_m(z) =
\begin{cases}
    z^2 & |z| < \varepsilon \\
    2 \varepsilon |z| - \varepsilon^2 & |z| \geq \varepsilon \\
\end{cases} \\
\end{gather*}
$$

$\sigma$ — константа масштабирования.

Рекомендуется устанавливать $\varepsilon = 1.35$ для достижения 95% статистической эффективности.

### Кластеризация. kMeans, kMeans++, MeanShift, DBSCAN.
**k-means:** количество кластеров предопределено. Кластер задаётся его **центроидом** (точкой). Плоскость (пространство) делится диаграммой Вороного (т.е. точка относится к тому кластеру, центроид которого к ней ближайший). Задача — минимизировать

$$
\sum_{\vec{x}_j \in X} \min_{\mu_i} \Bigl| |\vec{x}_j - \mu_i| \Bigr|_2^2
$$

То есть функция ошибки это сумма квадратов расстояний от точек до центроидов назначенных им кластеров.
На каждом шаге вычисляются кластеры, затем центроиды перемещаются в центры масс своих кластеров.
Инициализируются центроиды случайным образом. Цикл работает до сходимости.

Преимущества:
+ Быстрая сходимость

Недостатки:
- Инициализация кластеров
- См. картинки к лекции 3

**k-means++:**
1. Выбрать центроиды случайным образом из точек
2. Для каждой точки **x** посчитать расстояние до ближайшего центроида $M(\vec{x})$
3. Выбрать новый центроид с вероятностью, пропорциональной $M^2(\vec{x})$
4. Повторить (k - 1) раз шаги 2-3
5. Запустить k-means
Тогда кластеры инициализируются более удалёнными друг от друга.

**Mean Shift:** количество кластеров не предопределено.
$$
\begin{aligned}
\mu_i^0 & = \vec{x}_i \\
\mu_i^{t + 1} & = \frac{\sum_{\vec{x}_j \in N(\mu_i^t)} \text{RBF}(\vec{x}_j - \mu_i^t) \vec{x}_j}{\sum_{\vec{x}_j \in N(\mu_i^t)} \text{RBF}(\vec{x}_j - \mu_i^t)}
\end{aligned}
$$

Где $N(\mu_j^t)$ — окрестность $\mu_j^t$.
Radial Basis Function:

$$
\text{RBF}(d) = \exp\left( -c \Bigl| |d| \Bigr|_2^2 \right)
$$

**DBSCAN (Density-Based Spatical Clustering Applications with Noise):** количество кластеров не предопределено. В качестве начальных ядер кластеров точки с хотя бы $m$ других точек на расстоянии не более заданного $\varepsilon$. Затем кластеры объединяются, если их ядра находятся не дальше $\varepsilon$ друг от друга.

Преимущества:
+ Можно работать с не сферическими кластерами

Недостатки:
- Надо подбирать $\varepsilon$ и $m$
- Медленно работает

## Билет 2
### Смещение и дисперсия, понятие средней гипотезы.
### Ансамбли. Soft and Hard Voting. Bagging. Случайный лес. AdaBoost.

## Билет 3
### Линейная регрессия. LASSO, LARS. CART. SVR.
**Регуляризация линейной регрессии**
**L2** — к ошибке добавляется $\alpha W^T W$, т.е.

$$
L = (XW - Y)^T (XW - Y) + \alpha W^T W
$$

Тогда

$$
W = (X^T X + \alpha I)^{-1} X^T Y
$$

**L1**, она же **LASSO (Least Absolute Shrinkage and Selection Operator)** — к ошибке добавляется $\alpha ||W||$. Тогда искать решение надо численными методами, например, градиентным спуском или _LARS_.

**LARS (Least Angle Regression):**
1. Взять фичу $x_i$, максимально коррелирующую с $y$
2. Ввести $\beta_1$ как множитель для $x_i$ и увеличивать/уменьшать его, пока корреляция $x_i$ с $r = y - \hat{y}$ максимальна
3. Найти $x_j$ — новую фичу, максимально коррелирующую с $y$
4. Ввести $\beta_2$ как множитель для $(x_i \pm x_j)$
5. Повторять с шага 2, пока $\alpha \cdot \sum_i \beta_i < -\Delta \text{Error}$

**Elastic Net**

$$
L = ||XW - Y||_2^2 + \alpha ||W||_2^2 + \beta ||W||_1
$$

### Деревья решений. Информационный выигрыш. Ошибка классификации, энтропия, критерий Джини.

## Билет 4
### Глобальный поиск. Случайный поиск. Grid search. Случайное блуждание. Байесовская оптимизация. Кросс-энтропийный поиск.
### Линейная регрессия. Полиномиальная регрессия. Гребневая регрессия.
**Линейная регрессия:** минимизируем среднеквадратическое отклонение.
Можно дополнительно ввести $x_{i0} = 1$ и $w_0$ для удобства подсчёта смещения (bias). Тогда $\hat{M} = M + 1$.

$$
\begin{gather*}
\text{MSE} = \frac{1}{N} \sum_{i=1}^N (W^T \vec{x}_i - y_i)^2 = \frac{1}{N} \Bigl| |X W - Y| \Bigr|_2^2 \\
X =
\begin{bmatrix}
    1 & x_{11} & \cdots & x_{1M} \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{N1} & \cdots & x_{NM} \\
\end{bmatrix} \\
Y =
\begin{bmatrix}
    y_1 \\
    \vdots \\
    y_N \\
\end{bmatrix} \\
W =
\begin{bmatrix}
    w_0 \\
    w_1 \\
    \vdots \\
    w_M \\
\end{bmatrix} \\
L_\text{linear}(W) = N \cdot \text{MSE} = \Bigl| |X W - Y| \Bigr|_2^2 \\
\end{gather*}
$$

Минимизируем $\text{MSE} \sim L_\text{linear}$:

$$
\begin{gather*}
\begin{aligned}
L_\text{linear}(W)
& = (X W - Y)^T (X W - Y) = \\
& = (W^T X^T - Y^T) (X W - Y)
\end{aligned} \\
\nabla L_\text{linear}(W) = 2 X^T (X W - Y) = \vec{0} \\
X^T X W = X^T Y \\
W = (X^T X)^{-1} X^T Y \\
\end{gather*}
$$

Можно перейти к полиномиальной регрессии, просто добавив признаки $x_{ij}^q$. Но слишком много признаков приведут к переобучению.

**Смещение и дисперсия (bias & variance)**:

$$
\begin{gather*}
E_\text{out}(h^D) = \mathbb{E}_{\vec{x}} \left[ \left( h^D(\vec{x}) - f(\vec{x}) \right)^2 \right] \\
\begin{aligned}
\mathbb{E}_D \left[ E_\text{out}(h^D) \right]
& = \mathbb{E}_D \left[ \mathbb{E}_{\vec{x}} \left[ \left( h^D(\vec{x}) - f(\vec{x}) \right)^2 \right] \right] = \\
& = \mathbb{E}_X \left[ \mathbb{E}_D \left[ \left( h^D(\vec{x}) - f(\vec{x}) \right)^2 \right] \right] = \\
\end{aligned} \\
\overline{h}(\vec{x}) = \mathbb{E}_D \left[ h^D(\vec{x}) \right] \\
\begin{aligned}
& \mathbb{E}_D \left[ \left( h^D(\vec{x}) - f(\vec{x}) \right)^2 \right]
= \mathbb{E}_D \left[ \left( h^D(\vec{x}) - \overline{h}(\vec{x}) + \overline{h}(\vec{x}) - f(\vec{x}) \right)^2 \right] = \\
& = \mathbb{E}_D \left[ \left( h^D(\vec{x}) - \overline{h}(\vec{x}) \right)^2 + \left( \overline{h}(\vec{x}) - f(\vec{x}) \right)^2 + 2 \left( h^D(\vec{x}) - \overline{h}(\vec{x}) \right) \left( \overline{h}(\vec{x}) - f(\vec{x}) \right) \right] = \\
& = \mathbb{E}_D \left[ \left( h^D(\vec{x}) - \overline{h}(\vec{x}) \right)^2 \right] + \left( \overline{h}(\vec{x}) - f(\vec{x}) \right)^2 \\
\end{aligned} \\
\begin{aligned}
\mathbb{E}_D \left[ E_\text{out}(h^D) \right]
& = \mathbb{E}_X \left[ \mathbb{E}_D \left[ \left( h^D(\vec{x}) - f(\vec{x}) \right)^2 \right] \right] = \\
& = \mathbb{E}_X \left[ \mathbb{E}_D \left[ \left( h^D(\vec{x}) - \overline{h}(\vec{x}) \right)^2 \right] + \left( \overline{h}(\vec{x}) - f(\vec{x}) \right)^2 \right] = \\
& = \mathbb{E}_X \left[ \text{bias}(X) + \text{variance}(X) \right] = \\
& = \text{bias} + \text{variance}
\end{aligned} \\
\end{gather*}
$$

**Гребневая (Ridge) регрессия** — решение L2-регуляризации

$$
L = ||XW - Y||_2^2 + \alpha ||W||_2^2
$$

Тогда

$$
W = (X^T X + \alpha I)^{-1} X^T Y
$$

## Билет 5
### Градиентный бустинг решающих деревьев.
### Кластеризация. Agglomerative Clustering. Метрики кластеризации.
**Agglomerative clustering:** изначально каждая точка в своём кластере. Затем объединяются кластеры с наименьшим значением одной из метрик (обозначим кластеры как множества точек $A$ и $B$):
- Average — $\text{mean}(\rho(x, y)) \mid (x, y) \in A \times B$, среднее расстояние между точками в двух кластерах
- Single — $\min(\rho(x, y)) \mid (x, y) \in A \times B$, минимальное расстояние между точками в двух кластерах
- Complete — $\max(\rho(x, y)) \mid (x, y) \in A \times B$, максимальное расстояние между точками в двух кластерах
- Ward — дисперсия объединяемых кластеров
Цикл повторяется до достижения нужного количества кластеров.

Работает _очень_ медленно.

**Метрики кластеризации:**
- Внешние метрики
- **Homogenity score:**

$$
\frac{1}{|D|} \sum_i \max_y |\vec{x}_j \in C_i, y_j = y|
$$

- **Silhouette coefficient:** больше — лучше

$$
s = \frac{b - a}{\max(a, b)}
$$

$a$ — среднее расстояние между точкой и всеми остальными точками в её кластере

$b$ — среднее расстояние между точкой и всеми точками в ближайшем другом кластере

- **Dunn index:** больше — лучше

$$
D = \frac{\min_{i \neq j} \rho(\mu_i, \mu_j)}{\max_{\vec{x}_i, \vec{x}_j \in \mu} \rho(\vec{x}_i, \vec{x}_j)}
$$

- **Davies-Bouldin index:** меньше — лучше. $\overline{\rho(\mu_i, \vec{x}^i)}$ — среднее расстояние от центра до точек в $i$-м кластере

$$
\text{DB} = \frac{1}{k} \sum_{i=1}^k \max_{j \neq i} \left( \frac{\overline{\rho(\mu_i, \vec{x}^i)} + \overline{\rho(\mu_j, \vec{x}^j)}}{\rho(\mu_i, \mu_j)} \right)
$$

## Билет 6
### Оценка классификации. Эффективность по Парето. Precision-Recall и ROC кривые. AUC.
См. лекцию 2.

|             | Правильный ответ — True         | Правильный ответ — False |                                           |
|-------------|---------------------------------|--------------------------|-------------------------------------------|
| Ответ True  | True Positive                   | False Positive           | **Precision** = Positive Predictive Value |
| Ответ False | False Negative                  | True Negative            |                                           |
|             | **Recall** = True Positive Rate | False Positive Rate      | **Accuracy**                              |

$$
\begin{aligned}
\text{Precision} & = \frac{TP}{TP + FP} \\
\text{Recall} & = \frac{TP}{TP + FN} \\
\end{aligned}
$$

Precision (точность) и Recall (отклик) считаются отдельно для каждого класса, Accuracy (доля попаданий) — можно для всех вместе:

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**F1-score** — среднее гармоническое между Precision и Recall

$$
\begin{aligned}
F_1 = \frac{2}{\text{Precision}^{-1} + \text{Recall}^{-1}} = \frac{2 \cdot TP}{2 \cdot TP + FN + FP}
\end{aligned}
$$

Очевидно, $0 \leq F_1 \leq 1$.

**Pareto efficiency**: классификатор эффективен по Парето, если не существует классификатора лучше одновременно и по Precision, и по Recall. Увеличение одного приводит к уменьшению другого.

При повышении границы (threshold) повышается Precision, понижается Recall. На графике Precision / Recall двигаемся к увеличению точности и уменьшению отклика, на Recall / False Positive Rate — к уменьшению обоих.

**ROC (Receiver Operating Characteristic) Curve** — график Recall / False Positive Rate. **AUC** — Area Under \[ROC\] Curve. AUC — метрика классификатора, 0.5 — случайный классификатор, 1 — идеальный. Если инвертировать классификатор, то его AUC отразится от 0.5.

### Векторное представление слов. Word2Vec. Transformer.

## Билет 7
### Локальный поиск. Hill Climb и его разновидности. Отжиг. Генетический алгоритм.
### Метод опорных векторов. Прямая и двойственная задача. Решение двойственной задачи. Типы опорных векторов. Ядра.

## Билет 8
### Гипотезы и дихотомии. Функция роста. Точка поломки. Доказательство полиномиальности функции роста в присутствии точки поломки.
$E_\text{in}$ — ошибка в выборке. $E_\text{out}$ — ошибка вне выборки.

$$
\begin{aligned}
E_\text{in}(h) & = \frac{1}{N} \sum_{i=1}{N} e(h(\vec{x}_i), f(\vec{x}_i)) \\
E_\text{out}(h) & = \mathbb{E}_{\vec{x}} \left[ e(h(\vec{x}), f(\vec{x})) \right] \\
\end{aligned}
$$

**Hoeffding's inequality:** вероятность того, что разница между ошибкой в выборке и вне выборки превысит $\varepsilon$.

$$
P\left[ |E_\text{in}(h) - E_\text{out}(h)| > \varepsilon \right] \leq 2 \exp(-\varepsilon^2 N)
$$

Но если тестируем $M$ гипотез:

$$
P\left[ |E_\text{in}(h) - E_\text{out}(h)| > \varepsilon \right] \leq M \exp(-\varepsilon^2 N)
$$

Очевидно, ситуация ухудшается при $M \to +\infty$

- Гипотеза: $h : X \to \{ -1; 1 \}$
- Дихотомия: $h : \{ \vec{x}_1; \ldots; \vec{x}_N\} \to \{ -1; 1 \}$
- Всего дихотомий — не более $2^N$

**Функция роста** $m_H(N)$ — максимальное количество дихотомий. $m_H(N) \leq 2^N$.

$$
m_H(N) = \max_{\vec{x}_1; \ldots; \vec{x}_N} |H(\vec{x}_1, \ldots, \vec{x}_N)|
$$

**Неравенство Вапника-Червоненкиса:**

$$
P\left[ |E_\text{in}(h) - E_\text{out}(h)| > \varepsilon \right] \leq 4 m_H(2N) \exp \left(-\frac{\varepsilon N}{8} \right)
$$

Например, для двухмерного перцептрона и датасета с $X = \{ 0; 1 \}^2$ количество дихотомий будет $m_H = 14 < 16 = 2^{|X|}$. То есть для _двухмерного перцептрона_ **точка поломки** (где впервые появляется датасет с $m_H < 2^N$) — $k = 4$.

**Доказательство полиномиальности функции роста в присутствии точки поломки:**
- Рассматриваем множество $X = \{ x_1; \ldots; x_N \}$
- Количество дихотомий всего — $2^N$, не все из них возможны
- Пусть $B(N, k) = m_H(N)$ с точкой поломки $k$ — максимально возможное количество дихотомий такое, что ни для каких $k$ точек нет всех возможных $2^k$ дихотомий
    - $B(N, k) = \alpha + 2\beta$:
    - $\alpha$ таких, что дихотомия по $\{ \vec{x}_1; \ldots; \vec{x}_{N - 1} \}$ встречается один раз (не только в $\alpha$, а вообще уникальна)
    - Две по $\beta$ таких, что дихотомии по $\{ \vec{x}_1; \ldots; \vec{x}_{N - 1} \}$ не уникальны, а строки из разных $\beta$ отличаются только $h(\vec{x}_N)$
- Лемма: $\alpha + \beta \leq B(N - 1, k)$, т.к. в $\alpha + \beta$ содержатся все дихотомии на $\{ \vec{x}_1; \ldots; \vec{x}_{N - 1} \}$
- Лемма: $\beta \leq B(N - 1, k - 1)$. Пусть это не так. Тогда найдётся $2^{k - 1}$ дихотомий по $(k - 1)$ переменных в одной $\beta$. Тогда они же есть и во второй. Следовательно, добавив $x_N$, получаем набор с $2^k$ дихотомий по $k$ переменных в исходном $B(N, k)$, то есть противоречие.
- Следовательно, $B(N, k) \leq B(N - 1, k - 1) + B(N - 1, k)$.
- Следовательно, $B(N, k) \leq \sum_{i=0}^{k-1} \binom{N}{i}$:

$$
\begin{aligned}
B(N, 1) & = 1 \\
B(1, k) & = 2 \\
B(N, k)
& \leq B(N - 1, k) + B(N - 1, k - 1) \leq \\
& \leq \sum_{i=0}^{k-1} \binom{N - 1}{i} + \sum_{i=0}^{k-2} \binom{N - 1}{i} = \\
& = 1 + \sum_{i=1}^{k-1} \binom{N - 1}{i} + \sum_{i=1}^{k-1} \binom{N - 1}{i - 1} = \\
& = 1 + \sum_{i=1}^{k-1} \binom{N}{i} = \\
& = \sum_{i=0}^{k-1} \binom{N}{i}
\end{aligned}
$$

А $\sum_{i=0}^{k-1} \binom{N}{i}$ — полином.

**Размерность Вапника-Черваненкиса (VC-dimension)** — $d_\text{VC} = (k - 1)$, где $k$ — точка поломки. Например, для плоского перцептрона $d_\text{VC} = 3$. Тогда

$$
m_H(N) \leq \sum_{i=0}^{d_\text{VC}} \binom{N}{i} \leq N^{d_\text{VC}} + 1
$$

Для d-мерного перцептрона размерность Вапника-Черваненкиса — (d + 1). Доказательство — через обратимую матрицу:

$$
X =
\left.
\underbrace{
\begin{bmatrix}
1      &      0 & 0      & \cdots & 0      \\
1      &      1 & 0      & \cdots & 0      \\
1      &      0 & 1      & \cdots & 0      \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1      &      0 & 0      & \cdots & 1      \\
\end{bmatrix}
}_{d + 1}
\right\} d + 1
$$

Затем возьмём

$$
\begin{gather*}
\vec{x}_{d + 2} = \sum_{i=1}^{d + 1} \alpha_i \vec{x}_i \\
y_i = \text{sign}(\alpha_i) \\
y_{d + 2} = -1 \\
\end{gather*}
$$

Тогда $W^T \vec{x}_{d + 2} = \sum_{i=1}^{d + 1} (W^T \vec{x}_i \alpha_i \geq 0) \neq -1 = y_{d + 2}$, поскольку $W^T \vec{x_i} \cdot \text{sign}(\alpha_i) = 1$

Позволяет оценить, сколько данных будет достаточно:

$$
\begin{gather*}
4 m_H(2N) \exp \left(-\frac{\varepsilon N}{8} \right) \approx N^{d_\text{VC}} \exp(-N) \\
N \geq 10 d_\text{VC} \\
\end{gather*}
$$

### Деревья решений. Прунинг. Небрежные решающие деревья. Нечеткие решающие деревья.

## Билет 9
### Байесовский классификатор. Оценка признаков (Gaussian, Bernoulli, Multinomial). EM алгоритм.
### Нейронные сети. Перцептрон Розенблатта. Обратное распространение градиента. Функции активации. Softmax.

## Билет 10
### Сверточные нейронные сети. VGG. ResNet. Трансферное обучение.
### Метрические классификаторы. kNN. WkNN. Отбор эталонов. DROP5. KDtree.
**kNN (k Nearest Neighbors)** — ответ для точки определяется голосованием ответов её $k$ ближайших соседей из обучающей выборки. При этом для работы метода достаточно существования функции расстояния.

$$
\begin{gather*}
h(\vec{x}, D, k) = \arg\max_{y \in Y} \sum_{\vec{x}_i \in D} \left[ y_i = y \right] w(\vec{x}_i, X, k) \\
w(\vec{x}_i, x, k) = 1 \text{, если $\vec{x}_i$ --- один из $k$ ближайших соседей x} \\
\text{или} \\
w(\vec{x}_i, x, k) = 1 \text{, если $\rho(\vec{x}_i, x) \leq \text{радиус $k$-го соседа}$ } \\
\end{gather*}
$$

Метрика ошибки kNN: **leave-one-out error** — для каждой точки определяется её ответ, если её убрать из обучающей выборки, после этого считается точность (accuracy) классификации (т.к. функция ошибки, то инвертированная точность). Выбирается такое k, при котором достигается наилучшая точность (т.е. наименьшая ошибка).

$$
\text{LOO}(k, D) = \frac{\sum_{\vec{x}_i \in D}\left[ h(\vec{x}_i, D \backslash \vec{x}_i, k) \neq y_i \right]}{|D|}
$$

**WkNN (Weighted k Nearest Neighbors)** — вводится **ядро**, например

$$
\begin{aligned}
w_i & = \left[ \frac{r - \rho(\vec{x}, \vec{x}_i)}{r} \right]_+ \text{ --- треугольное ядро} \\
w_i & = q^{-\rho(\vec{x}, \vec{x}_i)} \text{ --- гауссово ядро} \\
\end{aligned}
$$

То есть убывающая неотрицательная функция от расстояния, в идеале — обращающаяся в 0 при достижении r. Ядро используется для вычисления весов в голосовании.

**Проклятие размерности:** рассмотрим распределённые в гиперкубе 0..1 точек. В 3-мерном кубе 0.1% точек попадает в куб со стороной 0.1, в то время как в 100-мерном — со стороной 0.93.
Решение: использовать пошаговый kNN, то есть отсортировать фичи и увеличивать количество рассматриваемых расстояний, пока точность улучшается.

**KDTree (k-dimensional Tree)** — для быстрого поиска ближайших соседей можно разбить пространство по медиане какой-нибудь координаты. Очевидно, что если расстояние от точки до "многомерного прямоугольника" больше, чем до найденного $k$-го ближайшего соседа, то улучшить результат какой-то точкой в этом прямоугольнике уже невозможно.
Такая эвристика позволяет многократно сократить время поиска ближайших соседей: сначала находим прямоугольник-лист, в который попадает точка, ищем соседей в нём, потом поднимаемся по дереву и ищем в нерассмотренных поддеревьях, сразу пропуская всё поддерево целиком, если расстояние слишком большое.
Разбиение ветки стоит делать по медиане координаты (для более сбалансированного дерева), и пока _наименьшее_ количество точек в поддереве больше заданного порога.

**Отбор эталонов (prototype selection):** иногда невозможно сохранить весь датасет (например, когда он большой), нужно сделать обучающую выборку по маленькой части. Есть разные способы выбрать, по какой именно.

**DROP5** — метод отбора эталонов:
1. Начать с полного датасета
2. Отсортировать точки по возрастанию близости до неправильного класса (по формуле kNN)
3. Пройти по отсортированному массиву. Точку **x** можно удалить, если это не приведёт к ухудшению **LOO** для тех точек, которые считают **x** одним из своих ближайших соседей