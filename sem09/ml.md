Лектор: Алексей Александрович Шпильман
Практик: Олег Свидченко

# Разная теория
**Обучение с учителем (supervised learning)** — на каждый вход из обучающей выборки известен правильный ответ.

**Train-test split** — выделяем часть выборки для *проверки*, насколько хорошо мы обучились. Тестовая выборка исключается из обучающей.

**Cross-validation** — проверка метрик на разных разбиениях на обучающую и тестовую выборки.

**One Hot Encoding** — каждый категориальный признак кодируется как вектор из нулей и одной единицы (количество измерений — количество возможных классов).

**Утечка данных** — протекание тестовой выборки в обучающую. Опасно тем, что метрика модели будет оцениваться неправильно, потому что может быть переобучение на *тестовых* данных.

## Точность классификации
(см. лекцию 2)

|             | Правильный ответ — True         | Правильный ответ — False |                                           |
|-------------|---------------------------------|--------------------------|-------------------------------------------|
| Ответ True  | True Positive                   | False Positive           | **Precision** = Positive Predictive Value |
| Ответ False | False Negative                  | True Negative            |                                           |
|             | **Recall** = True Positive Rate | False Positive Rate      | **Accuracy**                              |

$$
\begin{aligned}
\text{Precision} & = \frac{TP}{TP + FP} \\
\text{Recall} & = \frac{TP}{TP + FN} \\
\end{aligned}
$$

Precision (точность) и Recall (отклик) считаются отдельно для каждого класса, Accuracy (доля попаданий) — можно для всех вместе:

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**F1-score** — среднее гармоническое между Precision и Recall

$$
\begin{aligned}
F_1 = \frac{2}{\text{Precision}^{-1} + \text{Recall}^{-1}} = \frac{2 \cdot TP}{2 \cdot TP + FN + FP}
\end{aligned}
$$

Очевидно, $0 \leq F_1 \leq 1$.

**Pareto efficiency**: классификатор эффективен по Парето, если не существует классификатора лучше одновременно и по Precision, и по Recall. Увеличение одного приводит к уменьшению другого.

При повышении границы (threshold) повышается Precision, понижается Recall. На графике Precision / Recall двигаемся к увеличению точности и уменьшению отклика, на Recall / False Positive Rate — к уменьшению обоих.

**ROC (Receiver Operating Characteristic) Curve** — график Recall / False Positive Rate. **AUC** — Area Under \[ROC\] Curve. AUC — метрика классификатора, 0.5 — случайный классификатор, 1 — идеальный. Если инвертировать классификатор, то его AUC отразится от 0.5.

## Scalers
Масштабирование нужно для того, чтобы модель учитывала все фичи, а не только большие по модулю. Также может быть полезно центрировать фичу вокруг нуля.

MinMax scaler:

$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)} \cdot (\text{max}' - \text{min}') + \text{min}'
$$

MaxAbs scaler:

$$
x' = \frac{x}{\max(|x|)}
$$

Standard scaler:

$$
x' = \frac{x - \text{mean}(x)}{\text{std}(x)} = \frac{x - E(x)}{\sigma(x)}
$$

Robust scaler:

$$
x' = \frac{x - \text{median}(x)}{\text{quartile}_3(x) - \text{quartile}_1(x)}
$$


# Экзамен
## Билет 1
### Регрессия, борьба с выбросами. RANSAC. Theil-Sen. Huber.
### Кластеризация. kMeans, kMeans++, MeanShift, DBSCAN.

## Билет 2
### Смещение и дисперсия, понятие средней гипотезы.
### Ансамбли. Soft and Hard Voting. Bagging. Случайный лес. AdaBoost.

## Билет 3
### Линейная регрессия. LASSO, LARS. CART. SVR.
### Деревья решений. Информационный выигрыш. Ошибка классификации, энтропия, критерий Джини.

## Билет 4
### Глобальный поиск. Случайный поиск. Grid search. Случайное блуждание. Байесовская оптимизация. Кросс-энтропийный поиск.
### Линейная регрессия. Полиномиальная регрессия. Гребневая регрессия.

## Билет 5
### Градиентный бустинг решающих деревьев.
### Кластеризация. Agglomerative Clustering. Метрики кластеризации.

## Билет 6
### Оценка классификации. Эффективность по Парето. Precision-Recall и ROC кривые. AUC.
### Векторное представление слов. Word2Vec. Transformer.

## Билет 7
### Локальный поиск. Hill Climb и его разновидности. Отжиг. Генетический алгоритм.
### Метод опорных векторов. Прямая и двойственная задача. Решение двойственной задачи. Типы опорных векторов. Ядра.

## Билет 8
### Гипотезы и дихотомии. Функция роста. Точка поломки. Доказательство полиномиальности функции роста в присутствии точки поломки.
### Деревья решений. Прунинг. Небрежные решающие деревья. Нечеткие решающие деревья.

## Билет 9
### Байесовский классификатор. Оценка признаков (Gaussian, Bernoulli, Multinomial). EM алгоритм.
### Нейронные сети. Перцептрон Розенблатта. Обратное распространение градиента. Функции активации. Softmax.

## Билет 10
### Сверточные нейронные сети. VGG. ResNet. Трансферное обучение.
### Метрические классификаторы. kNN. WkNN. Отбор эталонов. DROP5. KDtree.
**kNN** (k Nearest Neighbors) — ответ для точки определяется голосованием ответов её k ближайших соседей из обучающей выборки. При этом для работы метода достаточно существования функции расстояния.

$$
h(\text{x}, D, k) = \arg\max_{y \in Y} \sum_{\text{x}_i \in D} \left[ y_i = y \right] w(\text{x}_i, x, k)
$$

$$
\begin{aligned}
w(\text{x}_i, x, k) & = 1 \text{, если $\text{x}_i$ --- один из $k$ ближайших соседей x} \\
\text{или} \\
w(\text{x}_i, x, k) & = 1 \text{, если $\rho(\text{x}_i, x) \leq \text{радиус $k$-го соседа}$ } \\
\end{aligned}
$$

Метрика ошибки kNN: **leave-one-out error** — для каждой точки определяется её ответ, если её убрать из обучающей выборки, после этого считается точность (accuracy) классификации (т.к. функция ошибки, то инвертированная точность). Выбирается такое k, при котором достигается наилучшая точность (т.е. наименьшая ошибка).

$$
\text{LOO}(k, D) = \frac{\sum_{\text{x}_i \in D}\left[ h(\text{x}_i, D \backslash \text{x}_i, k) \neq y_i \right]}{|D|}
$$

**WkNN** (Weighted k Nearest Neighbors) — вводится **ядро**, например

$$
\begin{aligned}
w_i & = \left[ \frac{r - \rho(\text{x}, \text{x}_i)}{r} \right]_+ \text{ --- треугольное ядро} \\
w_i & = q^{-\rho(\text{x}, \text{x}_i)} \text{ --- гауссово ядро} \\
\end{aligned}
$$

То есть убывающая неотрицательная функция от расстояния, в идеале — обращающаяся в 0 при достижении r. Ядро используется для вычисления весов в голосовании.

**Проклятие размерности:** рассмотрим распределённые в гиперкубе 0..1 точек. В 3-мерном кубе 0.1% точек попадает в куб со стороной 0.1, в то время как в 100-мерном — со стороной 0.93.
Решение: использовать пошаговый kNN, то есть отсортировать фичи и увеличивать количество рассматриваемых расстояний, пока точность улучшается.

**KDTree** (k-dimensional Tree) — для быстрого поиска ближайших соседей можно разбить пространство по медиане какой-нибудь координаты. Очевидно, что если расстояние от точки до "многомерного прямоугольника" больше, чем до найденного k-го ближайшего соседа, то улучшить результат какой-то точкой в этом прямоугольнике уже невозможно.
Такая эвристика позволяет многократно сократить время поиска ближайших соседей: сначала находим прямоугольник-лист, в который попадает точка, ищем соседей в нём, потом поднимаемся по дереву и ищем в нерассмотренных поддеревьях, сразу пропуская всё поддерево целиком, если расстояние слишком большое.
Разбиение ветки стоит делать по медиане координаты (для более сбалансированного дерева), и пока *наименьшее* количество точек в поддереве больше заданного порога.

**Отбор эталонов** (prototype selection): иногда невозможно сохранить весь датасет (например, когда он большой), нужно сделать обучающую выборку по маленькой части. Есть разные способы выбрать, по какой именно.

**DROP5** — метод отбора эталонов:
1. Начать с полного датасета
2. Отсортировать точки по возрастанию близости до неправильного класса (по формуле kNN)
3. Пройти по отсортированному массиву. Точку **x** можно удалить, если это не приведёт к ухудшению **LOO** для тех точек, которые считают **x** одним из своих ближайших соседей