Лектор: Алексей Александрович Шпильман
Практик: Олег Свидченко

# Разная теория
## Общие понятия
**Обучение с учителем (supervised learning)** — на каждый вход из обучающей выборки известен правильный ответ. Задача: аппроксимировать целевую функцию.

**Обучение без учителя (unsupervised learning)** — известны входные данные, но не выходные. Цели: извлечение информации, выделение зависимостей, сжатие данных.

**Train-test split** — выделяем часть выборки для _проверки_, насколько хорошо обучилась модель. Тестовая выборка исключается из обучающей.

**Cross-validation** — проверка метрик на разных разбиениях на обучающую и тестовую выборки.

**One Hot Encoding** — каждый категориальный признак кодируется как вектор из нулей и одной единицы (количество измерений — количество возможных классов).

**Утечка данных** — протекание тестовой выборки в обучающую. Опасно тем, что метрика модели будет оцениваться неправильно, потому что может быть переобучение на _тестовых_ данных.

## Точность классификации
(см. лекцию 2)

|             | Правильный ответ — True         | Правильный ответ — False |                                           |
|-------------|---------------------------------|--------------------------|-------------------------------------------|
| Ответ True  | True Positive                   | False Positive           | **Precision** = Positive Predictive Value |
| Ответ False | False Negative                  | True Negative            |                                           |
|             | **Recall** = True Positive Rate | False Positive Rate      | **Accuracy**                              |

$$
\begin{aligned}
\text{Precision} & = \frac{TP}{TP + FP} \\
\text{Recall} & = \frac{TP}{TP + FN} \\
\end{aligned}
$$

Precision (точность) и Recall (отклик) считаются отдельно для каждого класса, Accuracy (доля попаданий) — можно для всех вместе:

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**F1-score** — среднее гармоническое между Precision и Recall

$$
\begin{aligned}
F_1 = \frac{2}{\text{Precision}^{-1} + \text{Recall}^{-1}} = \frac{2 \cdot TP}{2 \cdot TP + FN + FP}
\end{aligned}
$$

Очевидно, $0 \leq F_1 \leq 1$.

## Scalers
Масштабирование нужно для того, чтобы модель учитывала все фичи, а не только большие по модулю. Также может быть полезно центрировать фичу вокруг нуля.

MinMax scaler:

$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)} \cdot (\text{max}' - \text{min}') + \text{min}'
$$

MaxAbs scaler:

$$
x' = \frac{x}{\max(|x|)}
$$

Standard scaler:

$$
x' = \frac{x - \text{mean}(x)}{\text{std}(x)} = \frac{x - E(x)}{\sigma(x)}
$$

Robust scaler:

$$
x' = \frac{x - \text{median}(x)}{\text{quartile}_3(x) - \text{quartile}_1(x)}
$$


## Другое

# Экзамен
## Билет 1
### Регрессия, борьба с выбросами. RANSAC. Theil-Sen. Huber.
### Кластеризация. kMeans, kMeans++, MeanShift, DBSCAN.
**k-means:** количество кластеров предопределено. Кластер задаётся его **центроидом** (точкой). Плоскость (пространство) делится диаграммой Вороного (т.е. точка относится к тому кластеру, центроид которого к ней ближайший). Задача — минимизировать

$$
\sum_{\text{x}_j \in X} \min_{\mu_i} \Bigl| |\text{x}_j - \mu_i| \Bigr|_2^2
$$

То есть функция ошибки это сумма квадратов расстояний от точек до центроидов назначенных им кластеров.
На каждом шаге вычисляются кластеры, затем центроиды перемещаются в центры масс своих кластеров.
Инициализируются центроиды случайным образом. Цикл работает до сходимости.

Преимущества:
+ Быстрая сходимость

Недостатки:
- Инициализация кластеров
- См. картинки к лекции 3

**k-means++:**
1. Выбрать центроиды случайным образом из точек
2. Для каждой точки **x** посчитать расстояние до ближайшего центроида $M(\text{x})$
3. Выбрать новый центроид с вероятностью, пропорциональной $M^2(\text{x})$
4. Повторить (k - 1) раз шаги 2-3
5. Запустить k-means
Тогда кластеры инициализируются более удалёнными друг от друга.

**Mean Shift:** количество кластеров не предопределено.
$$
\begin{aligned}
\mu_i^0 & = \text{x}_i \\
\mu_i^{t + 1} & = \frac{\sum_{\text{x}_j \in N(\mu_i^t)} \text{RBF}(\text{x}_j - \mu_i^t) \text{x}_j}{\sum_{\text{x}_j \in N(\mu_i^t)} \text{RBF}(\text{x}_j - \mu_i^t)}
\end{aligned}
$$

Где $N(\mu_j^t)$ — окрестность $\mu_j^t$.
Radial Basis Function:

$$
\text{RBF}(d) = \exp\left( -c \Bigl| |d| \Bigr|_2^2 \right)
$$

**DBSCAN (Density-Based Spatical Clustering Applications with Noise):** количество кластеров не предопределено. В качестве начальных ядер кластеров точки с хотя бы $m$ других точек на расстоянии не более заданного $\varepsilon$. Затем кластеры объединяются, если их ядра находятся не дальше $\varepsilon$ друг от друга.

Преимущества:
+ Можно работать с не сферическими кластерами

Недостатки:
- Надо подбирать $\varepsilon$ и $m$
- Медленно работает

## Билет 2
### Смещение и дисперсия, понятие средней гипотезы.
### Ансамбли. Soft and Hard Voting. Bagging. Случайный лес. AdaBoost.

## Билет 3
### Линейная регрессия. LASSO, LARS. CART. SVR.
### Деревья решений. Информационный выигрыш. Ошибка классификации, энтропия, критерий Джини.

## Билет 4
### Глобальный поиск. Случайный поиск. Grid search. Случайное блуждание. Байесовская оптимизация. Кросс-энтропийный поиск.
### Линейная регрессия. Полиномиальная регрессия. Гребневая регрессия.

## Билет 5
### Градиентный бустинг решающих деревьев.
### Кластеризация. Agglomerative Clustering. Метрики кластеризации.
**Agglomerative clustering:** изначально каждая точка в своём кластере. Затем объединяются кластеры с наименьшим значением одной из метрик (обозначим кластеры как множества точек $A$ и $B$):
- Average — $\text{mean}(\rho(x, y)) \mid (x, y) \in A \times B$, среднее расстояние между точками в двух кластерах
- Single — $\min(\rho(x, y)) \mid (x, y) \in A \times B$, минимальное расстояние между точками в двух кластерах
- Complete — $\max(\rho(x, y)) \mid (x, y) \in A \times B$, максимальное расстояние между точками в двух кластерах
- Ward — дисперсия объединяемых кластеров
Цикл повторяется до достижения нужного количества кластеров.

Работает _очень_ медленно.

**Метрики кластеризации:**
- Внешние метрики
- **Homogenity score:**

$$
\frac{1}{|D|} \sum_i \max_y |\text{x}_j \in C_i, y_j = y|
$$

- **Silhouette coefficient:** больше — лучше

$$
s = \frac{b - a}{\max(a, b)}
$$

$a$ — среднее расстояние между точкой и всеми остальными точками в её кластере

$b$ — среднее расстояние между точкой и всеми точками в ближайшем другом кластере

- **Dunn index:** больше — лучше

$$
D = \frac{\min_{i \neq j} \rho(\mu_i, \mu_j)}{\max_{\text{x}_i, \text{x}_j \in \mu} \rho(\text{x}_i, \text{x}_j)}
$$

- **Davies-Bouldin index:** меньше — лучше. $\overline{\rho(\mu_i, \text{x}^i)}$ — среднее расстояние от центра до точек в $i$-м кластере

$$
\text{DB} = \frac{1}{k} \sum_{i=1}^k \max_{j \neq i} \left( \frac{\overline{\rho(\mu_i, \text{x}^i)} + \overline{\rho(\mu_j, \text{x}^j)}}{\rho(\mu_i, \mu_j)} \right)
$$

## Билет 6
### Оценка классификации. Эффективность по Парето. Precision-Recall и ROC кривые. AUC.
См. слайды лекции 2.

**Pareto efficiency**: классификатор эффективен по Парето, если не существует классификатора лучше одновременно и по Precision, и по Recall. Увеличение одного приводит к уменьшению другого.

При повышении границы (threshold) повышается Precision, понижается Recall. На графике Precision / Recall двигаемся к увеличению точности и уменьшению отклика, на Recall / False Positive Rate — к уменьшению обоих.

**ROC (Receiver Operating Characteristic) Curve** — график Recall / False Positive Rate. **AUC** — Area Under \[ROC\] Curve. AUC — метрика классификатора, 0.5 — случайный классификатор, 1 — идеальный. Если инвертировать классификатор, то его AUC отразится от 0.5.

### Векторное представление слов. Word2Vec. Transformer.

## Билет 7
### Локальный поиск. Hill Climb и его разновидности. Отжиг. Генетический алгоритм.
### Метод опорных векторов. Прямая и двойственная задача. Решение двойственной задачи. Типы опорных векторов. Ядра.

## Билет 8
### Гипотезы и дихотомии. Функция роста. Точка поломки. Доказательство полиномиальности функции роста в присутствии точки поломки.
### Деревья решений. Прунинг. Небрежные решающие деревья. Нечеткие решающие деревья.

## Билет 9
### Байесовский классификатор. Оценка признаков (Gaussian, Bernoulli, Multinomial). EM алгоритм.
### Нейронные сети. Перцептрон Розенблатта. Обратное распространение градиента. Функции активации. Softmax.

## Билет 10
### Сверточные нейронные сети. VGG. ResNet. Трансферное обучение.
### Метрические классификаторы. kNN. WkNN. Отбор эталонов. DROP5. KDtree.
**kNN (k Nearest Neighbors)** — ответ для точки определяется голосованием ответов её $k$ ближайших соседей из обучающей выборки. При этом для работы метода достаточно существования функции расстояния.

$$
h(\text{x}, D, k) = \arg\max_{y \in Y} \sum_{\text{x}_i \in D} \left[ y_i = y \right] w(\text{x}_i, x, k)
$$

$$
\begin{aligned}
w(\text{x}_i, x, k) & = 1 \text{, если $\text{x}_i$ --- один из $k$ ближайших соседей x} \\
\text{или} \\
w(\text{x}_i, x, k) & = 1 \text{, если $\rho(\text{x}_i, x) \leq \text{радиус $k$-го соседа}$ } \\
\end{aligned}
$$

Метрика ошибки kNN: **leave-one-out error** — для каждой точки определяется её ответ, если её убрать из обучающей выборки, после этого считается точность (accuracy) классификации (т.к. функция ошибки, то инвертированная точность). Выбирается такое k, при котором достигается наилучшая точность (т.е. наименьшая ошибка).

$$
\text{LOO}(k, D) = \frac{\sum_{\text{x}_i \in D}\left[ h(\text{x}_i, D \backslash \text{x}_i, k) \neq y_i \right]}{|D|}
$$

**WkNN (Weighted k Nearest Neighbors)** — вводится **ядро**, например

$$
\begin{aligned}
w_i & = \left[ \frac{r - \rho(\text{x}, \text{x}_i)}{r} \right]_+ \text{ --- треугольное ядро} \\
w_i & = q^{-\rho(\text{x}, \text{x}_i)} \text{ --- гауссово ядро} \\
\end{aligned}
$$

То есть убывающая неотрицательная функция от расстояния, в идеале — обращающаяся в 0 при достижении r. Ядро используется для вычисления весов в голосовании.

**Проклятие размерности:** рассмотрим распределённые в гиперкубе 0..1 точек. В 3-мерном кубе 0.1% точек попадает в куб со стороной 0.1, в то время как в 100-мерном — со стороной 0.93.
Решение: использовать пошаговый kNN, то есть отсортировать фичи и увеличивать количество рассматриваемых расстояний, пока точность улучшается.

**KDTree (k-dimensional Tree)** — для быстрого поиска ближайших соседей можно разбить пространство по медиане какой-нибудь координаты. Очевидно, что если расстояние от точки до "многомерного прямоугольника" больше, чем до найденного $k$-го ближайшего соседа, то улучшить результат какой-то точкой в этом прямоугольнике уже невозможно.
Такая эвристика позволяет многократно сократить время поиска ближайших соседей: сначала находим прямоугольник-лист, в который попадает точка, ищем соседей в нём, потом поднимаемся по дереву и ищем в нерассмотренных поддеревьях, сразу пропуская всё поддерево целиком, если расстояние слишком большое.
Разбиение ветки стоит делать по медиане координаты (для более сбалансированного дерева), и пока _наименьшее_ количество точек в поддереве больше заданного порога.

**Отбор эталонов (prototype selection):** иногда невозможно сохранить весь датасет (например, когда он большой), нужно сделать обучающую выборку по маленькой части. Есть разные способы выбрать, по какой именно.

**DROP5** — метод отбора эталонов:
1. Начать с полного датасета
2. Отсортировать точки по возрастанию близости до неправильного класса (по формуле kNN)
3. Пройти по отсортированному массиву. Точку **x** можно удалить, если это не приведёт к ухудшению **LOO** для тех точек, которые считают **x** одним из своих ближайших соседей