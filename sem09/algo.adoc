= Билеты по алгоритмам
:language: Russian
:toc:
:source-highlighter: rouge
:source-language: julia
:stem: asciimath

== Теормин
Алгоритм.
Память и время как ресурсы.
Вычисление чисел Фибоначчи:
экспоненциальный рекурсивный алгоритм,
полиномиальный алгоритм.
O-символика как инструмент оценки ресурсов,
различные асимптотики (логарифм, полином, экспонента).
Все алгоритмы курса: оценки сложности и решаемые ими задачи.

=== Числа Фибоначчи

.Экспоненциальный алгоритм:
[source]
----
function fib(n)
    if n < 2
        n
    else
        fib(n - 1) + fib(n - 2)
    end
end
----

.Полиномиальный (линейный) алгоритм:
[source]
----
function fib(n)
   a = 0
   b = 1
   for i=1:n
       c = a + b
       a = b
       b = c
   end
   a
end
----

.Логарифмический алгоритм:
[source]
----
# Возведение матрицы в степень
using LinearAlgebra function fib(n)
   function helper(mat, pow)
       if pow <= 1
           mat
       else
           tmp = helper(mat, pow ÷ 2)
           tmp *= tmp
           if pow % 2 == 1
               tmp *= mat
           end
           tmp
       end
   end
   (helper([0 1; 1 1], n) * [0; 1])[1]
end
----

=== O-нотация

[stem]
++++
f in O(g) <=> exists C > 0, N | forall n >= N : f(n) < C * g(n)

f in Omega(g) <=> exists C > 0, N | forall n >= N : f(n) > C * g(n)

Theta(g(n)) = O(g) nn Omega(g)

f in cc "o"(g) <=> forall C > 0 exists N | forall n >= N : f(n) < C * g(n)

f in omega(g) <=> forall C > 0 exists N | forall n >= N : f(n) > C * g(n)
++++

== 1. Элементарные структуры данных
Массивы переменного размера: аддитивная и мультипликативная схемы реаллокации.
Односвязный список, двусвязный список.
Абстрактные типы данных, интерфейс и реализация.
Стек, очередь, дек; моделирование на основе массива.
Моделирование очереди с помощью двух стеков.
Амортизационный анализ: метод учётных стоимостей операций и метод предоплаты.

=== Массивы переменного размера
* Доступ к любому элементу за stem:[O(1)]
* Вставка в конец
* Удаление с конца

==== Аддитивная схема
Раз в несколько добавлений происходит переаллокация.
Сложность: начинаем с пустого массива,
stem:[k] реаллокаций раз в stem:[m] элементов.
Тогда время работы --
[stem]
++++
mk + sum_(i=0)^(k-1) mi = \
= m sum_(i=1)^k i = \
= m (k (k + 1)) / 2 in \
in O(mk^2) = O(n^2)
++++
Амортизированная сложность -- stem:[O(n)] на одну вставку в конец.

==== Мультипликативная схема
Переаллокация умножает размер массива.
Амортизированная сложность: начинаем с пустого массива,
размер каждый раз умножается на stem:[q], добавляем stem:[floor(q^k)] элементов.
Тогда время работы --
[stem]
++++
floor(q^k) + sum_(i=0)^(k-1) floor(q^i) = \
= sum_(i=0)^k floor(q^i) <= \
<= sum_(i=0)^k q^i = \
= 1 + q * (1 - q^k) / (1 - q) = \
= (q^(k+1) - 1) / (q - 1) in \
in O(q^k) = O(n)
++++
Амортизированная сложность -- stem:[O(1)] на одну вставку в конец.

=== Списки
==== Односвязный
* Доступ к первому элементу за stem:[O(1)]
* Вставка в любую точку за stem:[O(1)]
* Удаление из любой точки за stem:[O(1)]

==== Двусвязный
* Односвязный список + указатель назад
* Соединение за stem:[O(1)]

=== Абстрактные типы данных
==== Интерфейс
* Список допустимых операций
* Инварианты

==== Реализация
* Конкретные алгоритмы

==== Стек
* Вставка в начало
* Удаление из начала

==== Очередь
* Вставка в конец
* Удаление из начала

==== Дек
* Стек + очередь

=== Дек на массиве
* Массив переменного размера
* Номер начала
* Количество элементов
* Вставка -- если хватает места, то циклическое смещение итератора (при вставке в начало)
  и установка значения, затем смена количества элементов.
  Если места не хватает -- переаллокация массива.
* Удаление -- выбор значения либо по итератору, либо по циклическому смещению,
  затем смена количества элементов.
* Дек является и списком, и очередью

[source]
----
mutable struct Deque{T}
    arr :: Vector{T}
    first :: Int64
    size :: Int64
    Deque{T}() where T = new(Vector{T}(undef, 1), 1, 0)
end

function ensure_capacity!(deque :: Deque{T}, capacity :: Int64) where T
    length(deque.arr) < capacity || return
    new_arr = Vector{T}(undef, 2 * length(deque.arr))
    for i=1:deque.size
        new_arr[i] = deque.arr[(deque.first + i - 2) % length(deque.arr) + 1]
    end
    deque.arr = new_arr
    deque.first = 1
end

function push_back!(deque :: Deque{T}, x :: T) where T
    ensure_capacity!(deque, deque.size + 1)
    deque.arr[(deque.first + deque.size - 1) % length(deque.arr) + 1] = x
    deque.size += 1
end

function push_front!(deque :: Deque{T}, x :: T) where T
    ensure_capacity!(deque, deque.size + 1)
    deque.first = (deque.first + length(deque.arr) - 2) % length(deque.arr) + 1
    deque.arr[deque.first] = x
    deque.size += 1
end

function pop_back!(deque :: Deque{T}) where T
    deque.size -= 1
    deque.arr[(deque.first + deque.size - 1) % length(deque.arr) + 1]
end

function pop_front!(deque :: Deque{T}) where T
    x = deque.arr[deque.first]
    deque.first = deque.first % length(deque.arr) + 1
    deque.size -= 1
    x
end
----

=== Очередь на двух стеках
[source]
----
mutable struct Queue{T}
    left :: Deque{T}
    right :: Deque{T}
    Queue{T}() where T = new(Deque{T}(), Deque{T}())
end

function queue_push!(q :: Queue{T}, x :: T) where T
    push_back!(q.right, x)
end

function queue_pop!(q :: Queue{T}) where T
    if q.left.size == 0
        while q.right.size != 0
            push_back!(q.left, pop_back!(q.right))
        end
    end
    pop_back!(q.left)
end
----

=== Амортизационный анализ
Средняя стоимость операции за большое количество действий.

Пример: стек с операцией stem:["multipop"(n)]
-- для удаления stem:[n] элементов за stem:[O(n)] сначала их нужно добавить,
чему предшествуют stem:[n] операций stem:["push"(x)] за stem:[O(1)].
Всего -- stem:[n + 1] операция, поэтому амортизированная стоимость
-- stem:[(2n) / (n + 1) = O(1)].

Например, двоичный счётчик, где изменение 1 бита -- stem:[O(1)].
Тогда stem:[i]-й бит изменится stem:[n * 2^{-i}] раз,
всего на stem:[n] действий -- stem:[<= 2n = O(n)] времени,
следовательно, на одно действие -- stem:[O(1)] времени в среднем.

==== Метод потенциалов
Заведём stem:[Phi] -- потенциал.
После выполнения stem:[i] действий потенциал -- stem:[Phi_i].
Обозначим _стоимость_ операции stem:[alpha_i = t_i + Phi_i - Phi_{i - 1}].
Тогда если
[stem]
++++
{{:
[forall i : alpha_i in O(f(n, m))],
[forall i : Phi_i in O(n * f(n, m))]
:}:}
++++
то средняя амортизационная стоимость stem:[t in O(f(n, m))].

Доказательство:
[stem]
++++
a = 1/n sum_(i=1)^n t_i = \
= 1/n sum_(i=1)^n (alpha_i - Phi_i + Phi_{i - 1}) = \
= 1/n (sum_(i=1)^n alpha_i - sum_(i=1)^n Phi_i + sum_(i=1)^n Phi_{i - 1}) = \
= 1/n (sum_(i=1)^n alpha_i - sum_(i=1)^n Phi_i + sum_(i=0)^(n-1) Phi_i) = \
= 1/n (sum_(i=1)^n alpha_i - Phi_N + Phi_0) = \
= 1/n (sum_(i=1)^n O(f(n, m)) - O(n * f(n, m)) + O(n * f(n, m))) = \
= O(f, n)
++++

Пример: стек с stem:["multipop"(n)]:

* Потенциал -- количество элементов в стеке stem:[n in O(n * 1)]
* stem:[alpha("push") = 1 + Delta Phi = 2 in O(1)]
* stem:[alpha("pop") = 1 + Delta Phi = 0 in O(1)]
* stem:[alpha("multipop"(n)) = n + Delta Phi = 0 in O(1)]
Следовательно, амортизированная стоимость операций -- stem:[t in O(1)].

==== Метод предоплаты
Заводим учётные стоимости stem:[alpha_i] так, что
stem:[sum_(i=1)^n alpha_i >= sum_(i=1)^n t_i].
Тогда stem:[forall i : alpha_i in O(f) => a in O(f)].

Пример: стек с stem:["multipop"(n)].
Для stem:["push"] будем использовать 2 монеты,
тогда учётную стоимость удалений можно принять равной 0,
используя оставшуюся "лишнюю" монету после вставки.
Тогда stem:[a in O(f)].

== 2. Разделяй и властвуй
Рекуррентные соотношения.
Метод «разделяй и властвуй».
Умножение n-битовых чисел:
простой рекурсивный алгоритм,
улучшенный рекурсивный алгоритм.
Рекуррентные соотношения: основная теорема.
Двоичный поиск.

=== Метод «разделяй и властвуй»
Разбиваем задачу на подзадачи кратно меньшего размера.

=== Умножение n-битовых чисел
==== Простой рекурсивный алгоритм
Пусть stem:[X = 2^n a + b; Y = 2^n c + d] -- нижние и верхние половины,
каждая половина -- размера stem:[n].
[stem]
++++
X * Y = 2^(2n) * a * c + 2^n * (a * d + b * c) + c * d
++++
Тогда
[stem]
++++
{{:
[ T(1) = 1 ],
[ T(2n) = 4 T(n) + 4n ]
:}:}

T(n) = 3n^2 - 2n = O(n^2)
++++

==== Улучшенный рекурсивный алгоритм
Трюк Гаусса:
[stem]
++++
(a + bi) (c + di) = ac - bd + (ad + bc) i \
(a + b) (c + d) = ac + bd + ad + bc \
ad + bc = (a + b) (c + d) - ac - bd \

X = 2^n a + b \
Y = 2^n c + d \
X * Y = 2^(2n) ac + 2^n (ad + bc) + bd = \
= 2^(2n) ac + 2^n ((a + b)(c + d) - ac - bd) + bd
++++
То есть количество умножений сокращается с 4 до 3.
Алгоритм Карацубы.

[stem]
++++
{{:
[ T(1) = 1 ],
[ T(2n) = 3 T(n) + 8n ]
:}:}

T(2^k) = sum_(i=0)^k 3^i * 8 * 2^(k - i) = \
= 8 * 2^k * sum_(i=0)^k 3^i * 2^(-i) = \
= 8 * 2^k * sum_(i=0)^k (3/2)^i = \
= 8 * 2^k * (1 - (3/2)^(k + 1)) / (1 - 3/2) = \
= 16 * 2^k * ((3/2)^(k + 1) - 1)

T(n) = 16n * ((3/2)^(log_2 n + 1) - 1) = \
= O(n * (3/2)^(log_2 n)) = O(3^(log_2 n))
++++

=== Рекуррентные соотношения: основная теорема (master theorem)
[stem]
++++
T(n) = a * T(ceil(n / b)) + O(n^d)

a, b in NN, b > 1, d >= 0

a > b^d => T(n) in O(n^(log_b a))

a < b^d => T(n) in O(n^d)

a = b^d => T(n) in O(n^d log n)
++++

Доказательство:
на stem:[k]-м уровне рекурсии -- stem:[a^k * O((n / b^k)^d) действий.
Всего:
[stem]
++++
T(n) = sum_(k=0)^(log_b n) a^k * O((n / b^k)^d) = \
= O(n^d) * sum_(k=0)^(log_b n) (a / b^d)^k = \
= {
[O(n^d) * O(1), a < b^d],
[O(n^d) * O(log_b n), a = b^d],
[O(n^d) * O((a / b^d)^(log_b n)), a > b^d]
} = \
= {
[O(n^d), a < b^d],
[O(n^d * log n), a = b^d],
[O((n^d * a^(log_b n)) / (b^(log_b n))^d), a > b^d]
} = \
= {
[O(n^d), a < b^d],
[O(n^d * log n), a = b^d],
[O((n^d * a^(log_b n)) / (n^d)), a > b^d]
} = \
= {
[O(n^d), a < b^d],
[O(n^d * log n), a = b^d],
[O(a^(log_b n)), a > b^d]
}
++++

=== Двоичный поиск
Заводим предикат stem:[P(i) | forall j > i : P(i) -> P(j)],
т.е. он становится верным в какой-то точке, и во всех последующих он тоже верен.
Тогда можно завести stem:[l] и stem:[r], и, поддерживая инвариант
stem:[not P(l) and P(r)], найти точку смены значения за stem:[O(log(r - l))]:

. Находим stem:[m = (l + r) / 2]
. Если stem:[P(m)], то stem:[r := m]
. Иначе stem:[l := m]
. Повторяем, пока stem:[m notin {l, r}] (для целых чисел это будет stem:[l + 1 = r]) или до сходимости.

Теперь в stem:[l] -- самая правая точка, для которой предикат ещё не выполняется,
а stem:[r] -- самая левая, для которой выполняется.
Например, если stem:[P(i) = a\[i\] >= x], то stem:[a\[l\] < x; a\[r\] >= x].

== 3. Сортировка: квадратичные и сортировка слиянием
Квадратичные сортировки. Сортировка слиянием: с рекурсией и без.
Нижняя оценка stem:[Omega(n log n)] для сортировки сравнениями.

=== Квадратичные сортировки
* Пузырьком (элемент переставляется со следующим)
* Выбором
* Вставками -- хорошая константа

=== Сортировка слиянием
==== Рекурсивная
. Рекурсивно отсортировать левую и правую половины
. Слить их за stem:[O(n_i)]

Время работы:

* На одном "уровне слияния" -- ровно stem:[Theta(n)] действий
* Высота дерева -- stem:[Theta(log n)]
* Итоговая асимптотика -- stem:[Theta(n log n)]

==== Нерекурсивная
. Начинаем с подмассивов длины 1
. Переходим по длине stem:[n -> 2n] со слиянием stem:[2n - 1]-го и stem:[2n]-го соседей
. Повторяем в цикле, пока не будет единственный подмассив

=== Нижняя оценка для сортировки сравнениями
* Существует stem:[n!] возможных перестановок, и нужно выбрать одну из них всех
* Представим все возможные перестановки как листья дерева, в узлах которого -- сравнения
* Это будет stem:[k]-арное дерево, следовательно, его высота будет не меньше stem:[Omega (log_k (n!))]

[stem]
++++
Omega(log_k (n!)) = Omega(log (n!))

log (n!) = log (prod_(i=1)^n i) = \
= sum_(i=1)^n log i >= \
>= sum_(i=ceil(n/2))^n log ceil(n/2) = \
= ceil(n/2) * log ceil(n/2) >= \
>= n/2 * log (n/2) = \
= n/2 * (log n - log 2) >= \
>= [ n >= 4 ] >= n/4 * (log n - 1/2 log n) = \
= n/4 * log n = Omega(n log n)
++++

То есть любая сортировка сравнениями работает за stem:[Omega(n log n)],
что и требовалось доказать.

== 4. Сортировка кучей
Куча, построение кучи за линейное время.
Очередь с приоритетами на основе кучи.
Сортировка с помощью кучи, частичная сортировка.
Операции с d-ичной кучей.

=== Куча
* Дерево на массиве, индексация с 1
* Родитель stem:[k] имеет индекс stem:[floor((k - 1) / 2)]
* Инвариант: ключ в потомке не больше ключа в родителе (куча по максимуму)
* Просеивание вниз и вверх
** При просеивании вниз наверх вытягивается наибольший (в куче по максимуму) потомок
* Удаление -- через перестановку вершины с последним элементом и просеивание вниз новой вершины

=== Построение кучи за линейное время
* Начинаем с листьев, идём к корню
* Соединяем уже построенные кучи + элемент в кучу
** То есть для элемента stem:[i] сначала делаем кучи с корнями
   в stem:[2i] и stem:[2i + 1], а затем делаем
   SiftDown на stem:[i]
* Можно идти с конца до начала массива, но из-за кеширования лучше использовать обход в глубину

Время работы: stem:[T(2^(k + 1) - 1) = 2T(2^k - 1) + O(k)].
Можно заметить, что время работы не убывает от количества элементов.
Тогда stem:[T(n) <= 2 T ceil(n / 2) + O(log n) <= 2 T ceil(n / 2) + O(sqrt n)]

По основной теореме stem:[2 > sqrt 2 => T(n) in O(n^(log_2 2)) = O(n)]

=== Очередь с приоритетами на основе кучи
- См. операции с кучей

=== Сортировка с помощью кучи
. Построить кучу из всех элементов массива, stem:[O(n)]
. Извлекать по одному элементу из кучи и ставить на место, stem:[O(n * log n)]

Время работы -- stem:[O(n * log n)]

=== Частичная сортировка
* Нужно достать только первые stem:[k] порядковых статистик из stem:[n] элементов
* Строим кучу на первых stem:[k] элементах неотсортированного массива, stem:[O(k)]
* Проходим по всем оставшимся stem:[n - k] элементам массива, на каждом шаге:
*. Добавляем очередной элемент массива, stem:[O(log k)]
*. Удаляем вершину кучи (наибольший элемент), stem:[O(log k)]
* В конце остались stem:[k] наименьших элементов массива, и все в куче
* Сортируем их кучей, получаем stem:[k] упорядоченных наименьших элементов массива, stem:[O(k log k)]

Итого время работы: stem:[O(k) + (n - k) O(log k) + O(k log k) = O(k + n log k) = O(n log k)]

=== Операции с d-ичной кучей
* Посмотреть на вершину (максимум), stem:[O(1)]
* Извлечь вершину (максимум), stem:[O(log n)]
* Добавить элемент, stem:[O(log n)]
* Заменить ключ -- если поддерживать словарь,
  для чего достаточно сбалансированного дерева,
  то можно узнать положение ключа в куче за stem:[O(log n)].
  Если известно положение ключа, то можно этот ключ заменить или извлечь
  путём просеивания сначала вверх, затем вниз за stem:[O(log n)].
* Слияние куч (?)

== 5. Быстрая сортировка

Анализ среднего времени работы,
анализ глубины рекурсии,
элиминация хвостовой рекурсии,
IntroSort,
массивы с малым количеством различных элементов,
QuickSort3.

=== Анализ среднего времени работы
Предположим, что все ключи различны.
Первым pivot'ом массив разделяется на подмассивы длины stem:[i] и stem:[n - i - 1].
stem:[i] равновероятен от 0 до stem:[n - 1].
[stem]
++++
T(n) = O(n) + 1 / (n - 1) sum_(i=0)^(n - 1) (T(i) + T(n - i - 1)) = \
= O(n) + 2 / (n - 1) sum_(i=2)^(n - 1) T(i)
++++

Пусть stem:[alpha > 0] -- константа в stem:[O(n)].
Докажем, что stem:[exists beta > 0 | forall n >= 2 : T(n) <= beta n log n].
Очевидно, что для stem:[n = 2] утверждение выполняется.
Пусть оно выполнено stem:[forall N < n].
Рассмотрим stem:[n].
[stem]
++++
"Пусть" n' = floor(n / 2)

T(n) = O(n) + 2 / (n - 1) sum_(i=2)^(n - 1) T(i) <= \
<= alpha n + (2 beta) / (n - 1) sum_(i=2)^(n - 1) (i log i) = \
= alpha n + (2 beta) / (n - 1) (sum_(i=2)^(n') i log i + sum_(i=n' + 1)^(n - 1) i log i) <= \
<= alpha n + (2 beta) / (n - 1) (log n/2 * sum_(i=2)^n' i + log n * sum_(i=n' + 1)^(n - 1) i) = \
= alpha n + (2 beta) / (n - 1) (log n * sum_(i=2)^(n - 1) i - log 2 * sum_(i=2)^n' i) <= \
<= alpha n + (2 beta) / (n - 1) (log n * ((n + 1)(n - 2))/2 - log 2 * ((n' + 2)(n' - 1))/2) <= \
<= alpha n + beta (log n * (n + 1) - log 2 * ((n' + 2)(n' - 1)) / (n - 1)) <= \
<= alpha n + beta (log n * (n + 1) - log 2 * (((n-1)/2 + 2)((n-1)/2 - 1)) / (n - 1)) <= \
<= alpha n + beta (log n * (n + 1) - log 2 * ((n + 3)(n - 3)) / 4(n - 1)) <= \
<= alpha n + beta (log n * (n + 1) - log 2 * (n - 3) / 4) = \
= beta n log n + (alpha n + beta log n - beta (n - 3) / 4)
++++

При достаточно большом stem:[beta] слагаемое
stem:[alpha n + beta log n - beta (n - 3) / 4] будет отрицательным начиная с некоторого stem:[n].
Тогда stem:[exists beta > 0, N in NN | forall n >= N : T(n) <= beta n log n].
Очевидно, можно также подобрать stem:[beta] ещё больше, чтобы утверждение было верным
stem:[forall n >= 2].

=== Анализ глубины рекурсии
stem:[D(n)] -- математическое ожидание глубины рекурсии.
[stem]
++++
D(n) = 1 + 1 / (n - 1) sum_(i=0)^(n - 1) max(D(i), D(n - i - 1))
++++
Пусть stem:[exists beta : D(n) < beta * log n]
верно stem:[forall N < n].
Рассмотрим stem:[n]:
[stem]
++++
D(n)
= 1 + 1 / (n - 1) sum_(i=0)^(n - 1) max(beta * log i, beta * log(n - i - 1)) = \
= 1 + (2 beta) / (n - 1) sum_(i=ceil((n - 1) // 2))^(n - 1) log i <= \
<= 1 + beta / (n - 1) * (n - 1) * log n = \
= 1 + beta * log n in O(log n) \
++++
Аналогично, stem:[D(n) in O(log n)].

=== Элиминация хвостовой рекурсии
Второй рекурсивный вызов -- хвостовой.
Его можно преобразовать в цикл.
Поскольку рекурсивные вызовы независимы,
можно выполнить сначала тот, который будет на более коротком отрезке,
а затем сделать более длинный -- хвостовым.

=== IntroSort
Разделителем на каждом шаге выбирается медиана из трёх элементов массива
(например, левой и правой границ и середины массива).
При превышении глубины рекурсии stem:[c * log_2 n]
переходим от быстрой сортировки к сортировке с гарантированным stem:[O(n log n)],
например, сортировке кучей.

Преимущества:

* Гарантированно stem:[O(n log n)] по сравнению с обычной быстрой сортировкой, где в худшем случае stem:[O(n^2)]
* Небольшая константа, как и у быстрой сортировки
* Может тратить меньше памяти, чем сортировки с гарантированным stem:[O(n log n)]

=== Массивы с малым количеством различных элементов, QuickSort3
Отдельно выносим группу элементов, равных "поворотному",
тогда получается 3 отрезка с элементами
строго меньше, строго равными, и строго большими поворотного.
Очевидно, равные сортировать уже не нужно, и этот отрезок не пустой.

== 6. Линейные сортировки и порядковые статистики
Сортировка подсчётом, стабильность.
Цифровая сортировка.
Bucket sort для равномерно распределённых вещественных чисел.
Порядковые статистики, нахождение за линейное в среднем время.
Медиана медиан.

=== Сортировка подсчётом
Если сортируем целые числа из ограниченного stem:[O(n)] диапазона,
то можно посчитать количество каждого числа за stem:[O(n)],
затем восстановить уже отсортированный массив за stem:[O(n)].
Это не сортировка сравнением, поэтому не имеет stem:[Omega(n log n)],
и работает за stem:[O(n)].
[source]
----
function count_sort(arr)
    min_ = minimum(arr)
    max_ = maximum(arr)
    counts = fill(0, max_ - min_ + 1)
    for i=1:length(arr)
        counts[arr[i] - min_ + 1] += 1
    end
    i = 1
    for d = min_:max_
        for j=1:counts[d - min_ + 1]
            arr[i] = d
        end
        i += 1
    end
    arr
end
----

=== Стабильность
[source]
----
function count_sort_key(key, arr)
    min_ = minimum(key, arr)
    max_ = maximum(key, arr)
    counts = fill(0, max_ - min_ + 1)
    for e=arr
        counts[key(e) - min_ + 1] += 1
    end
    iters = fill(1, size(counts))
    iters[2:end] .+= cumsum(counts[1:end-1])
    sorted = similar(arr)
    for e=arr
        k = key(e) - min_ + 1
        sorted[iters[k]] = e
        iters[k] += 1
    end
    sorted
end
----

=== Цифровая сортировка
. Сортируем стабильным подсчётом младшие разряды
. Сортируем стабильным подсчётом старшие разряды
. И т.д. пока разряды не кончатся

[source]
----
function radix_sort(arr)
    for i=1:8
        arr = count_sort_key(n -> n ÷ 256^(i - 1) % 256, arr)
    end
    arr
end
----

Или:
. Сортируем старшие разряды
. Отрезки по старшим цифрам сортируем по младшим разрядам

Второй вариант можно использовать для лексикографической сортировки.

=== Bucket sort
При равномерном распределении чисел по отрезку можно разбить отрезок на "корзины,"
и каждую корзину отсортировать вставками.

[stem]
++++
bbb "E"[T(N)] = bbb "E" [sum_(i=1)^N O(n_i^2)]

bbb "E"[n_i] = 1 " по равномерному распределению"

bbb "E"[n_i^2] = bbb "D"[n_i] + bbb "E"^2 [n_i]

P[n_i = k] = binom(N)(k) p^k (1 - p)^k

p = 1/n

bbb "D"[n_i] = N p (1 - p) = N * 1/N * (1 - 1/N) = 1 - 1/N

bbb "E"[n_i^2] = bbb "D"[n_i] + bbb "E"^2 [n_i] = (1 - 1/N) + 1^2 = 2 - 1/N

bbb "E"[T(N)] = sum_(i=1)^N bbb "E"(n_i^2) = \
= sum_(i=1)^N (2 - 1/N) = \
= 2N - 1 in O(N)
++++

=== Порядковые статистики
stem:[k]-я порядковая статистика -- элемент,
который в отсортированном массиве будет стоять на stem:[k]-й позиции.

=== Поиск порядковой статистики
Можно заметить, что точка поворота в быстрой сортировке
встаёт на своё место при разделении массива.
Тогда нам точно известно, в каком подмассиве будет искомый элемент.
Тогда
[stem]
++++
bbb "E"[T(n, k)] = O(n) + 1/n * sum_(i=0)^(k-1) bbb "E"[T(n - i - 1)] + 1/n * sum_(i=k+1)^(n-1) bbb "E"[T(i)] <= \
<= O(n) + 1/n * sum_(i=ceil(n//2))^(n - 1) bbb "E"[T(i)] = O(n)
++++

=== Медиана медиан
. Разбиваем массив на отрезки по 5 элементов
. Находим медиану в каждом отрезке (stem:[O(1)] на каждом подотрезке,
  всего stem:[O(n)], т.к. количество элементов -- константа)
. Рекурсивно находим медиану от найденных медиан
. Точно знаем, что есть элементы, транзитивно не большие / не меньшие найденного,
  и их как минимум stem:[3 * floor(floor(n // 5) / 2) + 2].
  Осталось не более stem:[ceil((2n)/5)] элементов, которые могут быть медианой,
  причём медиана из них будет медианой массива.
  Дальше ищем рекурсивно

[stem]
++++
T(n) <= T(ceil(n/5)) + T(ceil((2n)/5)) + O(n) <= \
<= 2 T(ceil((2n)/5)) + O(n)

2 < (5/2)^1 => T(n) in O(n)
++++

== 7. Динамическое программирование 1
Общие принципы динамического программирования.
Кратчайшие пути в ациклических ориентированных графах.
Наибольшая возрастающая подпоследовательность:
подзадачи,
порядок на подзадачах,
граф подзадач,
сравнение с рекурсивным алгоритмом;
нахождение не только длины,
но и самой подпоследовательности.
Дискретная задача о рюкзаке.

=== Динамическое программирование
* Задача разбивается на подзадачи
* Ответы на позадачи имеет смысл запоминать

=== Кратчайшие пути в ациклических ориентированных графах
* Двигаемся из stem:[A] в stem:[B]
* Если до вершины stem:[C] мы можем добраться из вершин stem:[D_1, ..., D_m],
  то stem:[rho(A -> C) = min(rho(A -> D_1) + w(D_1 -> C), ..., rho(A -> D_m) + w(D_m -> C))]
. Отсортируем граф топологически
. stem:[forall i : rho(A -> v_i) := oo]
. stem:[rho(A -> A) := 0]
. Проходим по вершинам в топологическом порядке
. Если в вершине stem:[u] обнаруживаем stem:[rho(A -> u) + w(u -> v) < rho(A -> v)], то
  * stem:[rho(A -> v) := rho(A -> u) + w(u -> v)]
  * stem:["prev"(v) := u]
. Очевидно, когда достигли вершину stem:[u], уже рассмотрели все ведущие в неё рёбра
. Обратный путь -- односвязный список из stem:[B]

Очевидно, такой поиск пути работает за stem:[O(V + E)].

=== Наибольшая возрастающая подпоследовательность
* На входе последовательность stem:[a_1, ..., a_n]
* Нужно найти последовательность
  stem:[1 <= k_1 < ... < k_m <= n | m = max | a_(k_1) < ... < a_(k_m)],
  то есть stem:[forall 1 <= i < j <= m => k_i < k_j and a_(k_i) < a_(k_j)]

Представим последовательность как граф:
[stem]
++++
G = << V, E >>

V = { i in NN | i <= n }

E = { (i, j) in V^2 | i < j and a_i < a_j }
++++

==== Подзадачи
Поиск максимальной длины возрастающей подпоследовательности,
заканчивающейся заданным элементом:
[stem]
++++
L(j) = 1 + max{ {0} uu { L(i) | (i, j) in E } }
++++

Тогда ответ на всю задачу -- stem:[max_j L(j)].

==== Порядок на подзадачах
Порядок подзадач соответствует росту индексов.

==== Граф подзадач
См. выше.

==== Сравнение с рекурсивным алгоритмом
Построение графа подзадач -- stem:[O(n^2)],
после этого проход по графу -- stem:[O(V + E) in O(n^2)].
То есть динамическое решение -- stem:[O(n^2)].

Рекурсивное решение -- входит или не входит каждый конкретный элемент
в последовательность, в худшем случае -- stem:[O(2^n)].

==== Нахождение самой подпоследовательности
В каждой вершине записываем не только максимальную длину пути,
но и предыдущую вершину.

[source]
----
function max_subseq(arr)
    n = length(arr)
    len = fill(1, n)
    prev = fill(0, n)
    for i=2:n
        for jj=2:i
            j = jj - 1
            arr[j] < arr[i] || continue
            len[j] < len[i] && continue
            len[i] = 1 + len[j]
            prev[i] = j
        end
    end

    path = []
    v = argmax(len)
    while v != 0
        push!(path, v)
        v = prev[v]
    end
    reverse!(path)
    path
end
----

=== Дискретная задача о рюкзаке
Есть объекты с целым весом stem:[w_i] и вещественной ценой stem:[v_i].
Нужно положить в рюкзак вместимости stem:[W] максимальную стоимость stem:[V].

==== С повторениями
Подзадача -- вместимость stem:[W'].
[stem]
++++
V(W') | W' <= 0 = 0

V(W') = max_i { v_i + V(W' - w_i) }
++++

Очевидно, время работы динамического решения -- stem:[O(W * n)].
Используемая память -- stem:[O(W)].

==== Без повторений
Подзадача -- рюкзак вместимости stem:[W'], первые stem:[i] предметов.
Очередной товар либо берём, либо не берём.
[stem]
++++
V(W', i) | W' <= 0 = 0

V(W', 0) = 0

V(W', i) = max{
    [      V(W'       , i - 1)],
    [v_i + V(W' - w_i , i - 1)]}
++++

Ответ -- stem:[V(W, n)].
Время работы -- stem:[O(W * n)].

Поскольку мы не уходим дальше stem:[i - 1],
то достаточно хранить всего два столбца.
Если идти по уменьшению stem:[W'], то вообще достаточно одного.

Поэтому требуемая память -- stem:[O(W)].

== 8. Динамическое программирование 2
Умножение матриц.
Независимые множества максимального веса в деревьях.
Редакционное расстояние:
граф на подзадачах,
нахождение кратчайшего пути в данном графе;
вычисление редакционного расстояния с использованием линейной памяти (алгоритм Хиршберга).

=== Умножение матриц
Известно, что матричное умножение _ассоциативно_: stem:[A xx (B xx C) = (A xx B) xx C].
При этом перемножение матриц размера stem:[M xx K] и stem:[K xx N]
-- это матрица размера stem:[M xx N], и её вычисление занимает stem:[M xx N xx K] времени.
Нужно выбрать наилучшую последовательность умножений.

Пусть мы перемножаем stem:[n + 1] матрицу, т.е. происходит stem:[n] умножений,
stem:[i]-я матрица имеет размер stem:[M_i xx M_(i + 1)].

Можно представить результат как двоичное дерево,
где листья -- исходные матрицы,
а узлы -- операции умножения.
Если результат оптимален, то и его поддеревья оптимальны.

Подзадача -- оптимизация произведения идущих подряд матриц:
stem:[C(l, r)] -- минимальная стоимость вычисления stem:[A_l xx ... xx A_r].
Тогда
[stem]
++++
C(l, r) = min_{l <= i < r} { C(l, i) + C(i + 1, r) + M_l * M_(i + 1) * M_(r + 1) }
++++

Тогда алгоритм:
[source]
----
function best_matprod(sizes)
    n = length(sizes) - 1
    cost = fill(typemax(Int64) ÷ 2, (n, n))
    best = fill(0, (n, n))
    for i=1:n
        cost[i, i] = 0
    end
    for step=1:n-1
        for l=1:n
            r = l + step
            r > n && break
            for i=l:r-1
                tmp = cost[l, i] + cost[i + 1, r] + sizes[l] * sizes[i + 1] * sizes[r + 1]
                tmp < cost[l, r] || continue
                cost[l, r] = tmp
                best[l, r] = i
            end
        end
    end
    cost, best
end
----

Работает, очевидно, за stem:[O(n^3)] по времени и stem:[O(n^2)] по памяти.

=== Независимые множества максимального веса в деревьях
Множество вершин называется _независимым_, если его вершины не соединены рёбрами.

Динамика: для поддерева запоминаем ответ, когда корень брать разрешено (но он не обязательно взят),
и когда его брать запрещено.

=== Редакционное расстояние
На входе две строки (массивы символов).
Элементарные операции за stem:[O(1)]:
* Вставить символ
* Заменить символ
* Удалить символ
Редакционное расстояние -- это количество элементарных операций,
которые нужно совершить, чтобы преобразовать одну строку в другую.

==== Граф на подзадачах
Скажем, что подзадача stem:[rho(i -> j)] --
расстояние между префиксами строк длин stem:[i] и stem:[j] соответственно.
Тогда:
[stem]
++++
rho(i -> 0) = i

rho(0 -> j) = j

rho(i -> j) = min{
[rho(i - 1 -> j - 1)     ,|, s_1[i] = s_2[j]              ],
[rho(i - 1 -> j - 1) + 1 ,|, s_1[i] != s_2[j] " — замена" ],
[rho(i - 1 -> j    ) + 1 ,|, "удаление"                   ],
[rho(i     -> j - 1) + 1 ,|, "вставка"                    ]
}
++++

Очевидно, построение такого графа -- stem:[O(n * m)]
по времени и памяти.

==== Нахождение кратчайшего пути в графе
Можно дополнительно в каждой вершине запоминать, откуда мы в неё пришли.

==== Линейная память
Можно заметить, что мы идём не дальше stem:[i - 1] и stem:[j - 1],
поэтому можно вместо всей матрицы хранить только две строки/столбца
(в зависимости от того, что меньше).

Тогда требуемая память -- stem:[O(min(n, m))].

Но так теряется обратный путь.

==== Алгоритм Хиршберга
Приходим к середине одной строки по префиксам и суффиксам.
То есть stem:[rho'(i -> j)] -- расстояние между суффиксами
строк длины stem:[i] и stem:[j] соответственно.
Можно также сказать, что stem:[rho'(s_1 -> s_2) = rho("reverse"(s_1) -> "reverse"(s_2))].
Тогда stem:[rho(n -> m) = min_k { rho(floor(n/2) -> k) + rho'(ceil(n/2) -> m - k) }]

Тогда известно нужное редактирование в середине stem:[s_1],
можно рекурсивно делить stem:[s_1] пополам до строк длины 1,
и из этого получить последовательность редактирования.

[stem]
++++
T(n, m) = O(nm) + T(floor(n/2), k) + T(ceil(n/2), m - k)

T(n, m) in O(nm)
++++

== 9. Жадные алгоритмы 1
Покрытие точек единичными отрезками.
Непрерывный рюкзак.
Задача о выборе заявок.
Максимальные независимые множества в деревьях.
Код Хаффмана.

=== Покрытие точек единичными отрезками
Даны точки на прямой.
Нужно покрыть их минимальным количеством единичных отрезков.

Очевидно, если отсортировать точки, и затем для каждой ещё не покрытой
добавлять отрезок, для которого эта точка будет левой границей,
то в итоге будут покрыты все точки, причём минимальным количеством отрезков.
stem:[O(n log n)] из-за сортировки или stem:[O(n)],
если точки заранее отсортированы.

=== Непрерывный рюкзак
В отличие от дискретного рюкзака, товары можно дробить
(условно, золотой песок вместо золотых слитков).
Тогда стоит отсортировать товары по соотношению цена/вес,
и брать максимально возможное количество товара с наилучшим соотношением.

=== Задача о выборе заявок
Известен список заявок, которые нужно начать делать в заданный момент или отказать.
Известно время обработки заявки.
Нужно максимизировать количество обработанных заявок.

Нужно брать заявку, конец выполнения которой наступит раньше всего.

=== Максимальные независимые множества в деревьях
Максимизируем поддеревья узла, затем, если возможно, берём узел.
Можно сформулировать то же решение иначе:
. Берём все листья
. Убираем листья и их предков из дерева
. Повторяем до конца

=== Код Хаффмана
Дана строка, которую нужно закодировать минимальным количеством битов.
Требования:

* Однозначность
* Префиксный код

Получится бинарное дерево, где левое ребро -- 0, правое -- 1, в листе -- символ.
Длина кода символа равна расстоянию от соответствующего листа до корня.

Будем в каждую вершину stem:[v] (в т.ч. листья stem:[l]) записывать,
как часто встречается её поддерево stem:[n(v)].
Цена дерева: stem:[sum_l n(l) * h(l) = sum_v n(v) - n_"root"].

* В оптимальном дереве нет родителей одного ребёнка.
  Если такой находится, то можно вытянуть его ребёнка,
  тем самым удалив одну вершину, и строго улучшить ответ.
* Два листа с наименьшими частотами находятся на нижнем уровне.
  Если это не так, то есть пара листьев такая,
  что лист большей частоты находится на уровне ниже.
  Тогда можно переставить их местами и строго улучшить ответ.
* Существует оптимальное дерево, в котором два листа
  наименьшей частоты -- братья.

Алгоритм Хаффмана по построению оптимального дерева кодирования:

. Завести приоритетную очередь по минимуму
. Добавить в приоритетную очередь все листья (символ + частота)
. Пока в очереди больше 1 элемента:
.. Забрать 2 минимальных вершины из приоритетной очереди
.. Добавить в приоритетную очередь их объединение (вершины как дети, частота -- сумма частот детей)
. Вернуть вершину приоритетной очереди

== 10. Жадные алгоритмы 2
Минимальное покрывающее дерево:
свойство разреза,
жадная стратегия,
алгоритм Прима,
алгоритм Краскала.

=== Минимальное покрывающее дерево
Minimum Spanning Tree -- дерево, состоящее из всех вершин графа
и части его рёбер, имеющее минимальную сумму весов рёбер.

=== Свойство разреза
Пусть stem:[M] -- MST в графе stem:[G = << V; E >>].
Пусть stem:[S_1 uu S_2 = V] -- разрез stem:[G].
Пусть stem:[T sub M] не содержит рёбер через разрез.
Тогда если stem:[{u; v}] -- минимальное ребро в разрезе,
то существует MST stem:[M' | T uu {e} sub M'].

==== Доказательство
Достроим stem:[T] до какого-нибудь MST stem:[M^**].
Если stem:[{u; v} in M^**], то искомое stem:[M' = M^**] найдено.

В противном случае рассмотрим путь между stem:[u] и stem:[v].
Очевидно, он пересекает разрез по какому-то ребру stem:[e != {u; v}].
Тогда если добавить ребро stem:[{u; v}], то образуется цикл.

Тогда можно удалить любое другое ребро этого цикла без потери связности.
Если мы удалим ребро stem:[e], по которому был пересечён разрез,
то мы гарантированно не испортим ответ,
т.к. stem:[w({u; v}) <= w(e)] по постановке задачи.
Следовательно, stem:[M' = M^** \\ {e} uu {{u; v}}] -- искомое MST.

=== Жадная стратегия
Добавлять минимальные возможные рёбра через разрезы.

=== Алгоритм Прима
. Поддерживаем приоритетную очередь:
  * Элемент -- вершина, которую можно добавить в дерево за одно ребро, и само это ребро
  * Ключ -- вес этого ребра
. Добавляем в очередь какую-нибудь вершину, с нулевым весом фиктивного ребра
. Пока не построили MST:
.. Достаём вершину из очереди
.. Добавляем её в MST
.. Релаксируем все её рёбра

Очевидно, на каждое ребро мы посмотрим ровно два раза,
каждую вершину -- достали из очереди ровно один раз.

Приоритетную очередь можно поддерживать двумя способами:

* Через кучу
** Добавление -- stem:[O(log V)]
** Обновление ключа -- stem:[O(log V)]
** Извлечение -- stem:[O(log V)]
** Всего -- stem:[O(E log V)]
* Через массив: на каждом шаге искать вершину из всех вершин графа
** Добавление -- stem:[O(1)]
** Обновление ключа -- stem:[O(1)]
** Извлечение -- stem:[O(V)]
** Всего -- stem:[O(V^2 + E) = O(V^2)]

Можно заметить, что stem:[E = O(V^2)],
поэтому стоит выбирать, использовать кучу или массив,
в зависимости от плотности графа.

=== Алгоритм Краскала (Kruskal)
. Отсортировать все рёбра по весу
. Пройти по рёбрам в порядке возрастания веса
  * Если ребро соединяет разные компоненты связности (см. СНМ),
    то взять его и объединить компоненты связность

Время работы -- stem:[O(E log E)] из-за сортировки,
без неё -- stem:[O(E log^** V)].

== 11. Система непересекающихся множеств
Представление множеств с помощью деревьев,
эвристики: ранги и сжатие путей,
верхняя оценка stem:[O(m log^** n)] на время работы stem:[m] операций.
Анализ учётных стоимостей операций: метод ростовщика.

=== Представление множеств с помощью деревьев
Элементы -- вершины, множества -- деревья.
У вершины -- указатель на родителя.

Поиск, в каком множестве находится элемент -- это поиск корня соответствующего дерева.

Объединение множеств -- подвесить корень одного дерева к другому.

=== Ранги
Можно добавить каждой вершине ранг -- высоту её поддерева.
Подвешивание происходит только к корню, поэтому ранг нужно обновлять только у него.

Докажем, что у дерева с корнем ранга stem:[r] хотя бы stem:[2^r] вершин:

* Очевидно, при ранге 0 у дерева stem:[1 >= 2^0] вершина.
* Корень с рангом stem:[r + 1] получается либо подвешиванием к корню с рангом stem:[r + 1]
  корней меньшего ранга, либо объединением двух корней ранга stem:[r].

Тогда по индукции у дерева с корнем ранга stem:[r] хотя бы stem:[2^r] вершин.
Следовательно, максимальный ранг -- не более stem:[log_2 n],
тогда поиск корня занимает stem:[O(log n)] времени.

=== Сжатие путей
* Поиск корня -- рекурсивный
* После поиска вершина переподвешивается к корню
* Ранги больше не высота поддерева
* Ранг родителя всё ещё строго больше ранга потомка
* Если вершина перестаёт быть корнем, то её ранг больше не изменяется

=== Верхняя оценка на время работы
Разобьём отрезок stem:[\[1; log n\]] на отрезки вида stem:[\[k + 1; 2^k\]]:
stem:[{1}, {2}, \[3; 4\], \[5; 16\], \[17; 2^16\], ...]

Скажем, что красные рёбра -- те, на которых ранг "перепрыгивает" в другой отрезок,
а чёрные -- те, на которых ранг остаётся прежним.

На любом пути красных рёбер не более stem:[log^** n], поскольку столько отрезков всего.

Вершин с рангом stem:[k] не более stem:[n // 2^k],
поскольку каждая новая вершина этого ранга -- корень дерева.

Тогда в интервале stem:[\[k + 1; 2^k\]] находится
не более stem:[sum_(i=k+1)^(2^k) n / 2^i] вершин.

Тогда всего не корневых переходов по чёрным рёбрам (во всех поддеревьях) в этом интервале
stem:[sum_(i=k+1)^(2^k) n / 2^i * 2^k = n * 2^k * sum_(i=k+1)^(2^k) <= n * 2^k * 2^(-k) = n].

Тогда всего не корневых переходов по чёрным рёбрам не более stem:[n * log^** n]
(поскольку stem:[log^** n] -- число интервалов).

Пусть выполнено stem:[m] операций поиска корня.
Тогда всего переходов:
stem:[m * O(log^** n) + O(n * log^** n) + O(m)]
-- красные, чёрные и корневые рёбра.
Если stem:[m > n], то всего переходов stem:[O(m * log^** n)],
тогда амортизированная цена запроса -- stem:[O(log^** n)].

== 12. Декомпозиция графов 1
Графы и способы их представления:
матрица смежности,
списки смежности,
матрица инцидентности.
Поиск в глубину.
Графы и способы их представления,
способы использования графов.
Поиск в глубину в неориентированных графах,
выделение компонент связности,
нахождение циклов.
Поиск в глубину в ориентированных графах: поиск цикла.

=== Способы представления графов
Граф stem:[G = << V; E >>].

==== Матрица смежности
* Проверка рёбер за stem:[O(1)]
* Размер stem:[O(V^2)]
* Перебор рёбер из вершины за stem:[O(V)]

==== Списки смежности
* Размер stem:[O(V + E)]
* Перебор соседей за stem:[O("количество соседей")]
* Проверка ребра за stem:[O(V)]

==== Матрица инцидентности
Матрица инцидентности вершина-ребро.

* Размер stem:[O(V + E) sub O(V^3)]
* Строка соответствует вершине, столбец -- ребру
* Сумма столбца не больше 2
* Сумма строки равна степени вершины
* Применимо для петель

=== Поиск в глубину
. Время входа
. Рекурсия по соседям
. Время выхода

Свойства:

* По-английски -- Depth First Search, DFS
* Работаем на списках смежности
* От одной вершины -- stem:[O(E)]
* Обычно запускают на всех вершинах последовательно, тогда -- stem:[O(V + E)]
* На матрице смежности -- stem:[O(V^2)]

=== Способы использования графов
* Представление пространства, поиск пути

=== Выделение компонент связности в неорграфах
. Запускаем DFS на одной вершине
  * Красим одним цветом все вершины, которых достигли впервые
. Выбираем новый цвет
. Повторяем, пока не кончатся вершины

Все вершины, принадлежащие одной компоненте связности,
будут покрашены одним цветом.
Можно было бы использовать СНМ, но нет необходимости.

=== Поиск циклов в неорграфах
* Рёбра обхода -- по ним проходит DFS
* Прямые рёбра -- из предка в вершину
* Обратные рёбра -- из вершины в предка (в неорграфах -- не в родителя)
* Запускаем DFS
** Если нашли обратное ребро, то оно входит в цикл
** Любой цикл содержит обратное ребро, т.к. перекрёстных рёбер нет
* Время работы -- stem:[O(V + E)]

=== Поиск циклов в орграфах
* Есть перекрёстные рёбра -- в другую ветку обхода
* Какое ребро -- зависит от конкретного обхода
* Понять тип ребра можно по времени входа/выхода в вершину:
** Время входа и выхода инициализируется stem:[+oo]
** При входе устанавливается время входа по таймеру
** При выходе устанавливается время выхода по таймеру
** Интервал вершины включается в интервал её родителя
** Интервалы вершин, не связанных отношением предок/потомок, не пересекаются
* Запускаем DFS
** Цикл содержит обратное ребро
* Время работы -- stem:[O(V + E)]

== 13. Декомпозиция графов 2
Поиск в глубину в ориентированных графах:
топологическая сортировка вершин,
выделение компонент сильной связности в орграфах.

=== Топологическая сортировка вершин
. Запускаем DFS
. Если обнаружили цикл -- выходим с ошибкой
. При выходе из вершины дописываем её в массив
. В конце разворачиваем массив, чтобы стоки были справа, а истоки -- слева

Тогда:

* Обратные рёбра недопустимы
* Рёбра обхода -- слева направо
* Прямые рёбра -- слева направо
* Перекрёстные рёбра появляются, когда выход уже добавлен в массив,
  поэтому при добавлении входа в массив перекрёстные рёбра тоже будут слева направо
* Время работы -- stem:[O(V + E)]

=== Поиск компонент сильной связности в орграфах
Транспонирование графа -- обращение рёбер.
Метаграф -- DAG (Directed Acyclic Graph) компонент связности заданного графа.

Если есть ребро между компонентами сильной связности stem:[C -> C'], то:

* Нет ребра stem:[C' -> C], иначе это одна компонента сильной связности
* Если раньше вошли в stem:[C], тогда в stem:[C'] войдём в процессе обхода stem:[C],
  и выйдем из stem:[C] позже, чем из stem:[C']
* Если раньше вошли в stem:[C'], тогда в stem:[C] войдём после выхода из stem:[C'],
  и выйдем из stem:[C] позже, чем из stem:[C']
* То есть всегда время выйдем из stem:[C] позже, чем из stem:[C']

Алгоритм:

. Запустим DFS на всех вершинах
. При выходе из вершины добавляем её в массив заранее известного размера,
  тогда сортировка вершин по убыванию времени выхода -- stem:[O(V + E)]
. Вершина stem:[u] с наибольшим временем выхода будет принадлежать истоку
  * В транспонированном графе -- стоку
. Найдём вершины stem:[v], из которых она достижима
  * Для этого запустим DFS из stem:[u] на транспонированном графе
. Все эти stem:[v] принадлежат той же компоненте сильной связности, что и stem:[u]
  * Удалим их из графа
. Найдём оставшуюся вершину с наибольшим временем выхода,
  повторяем, пока есть вершины

Время работы -- stem:[O(V + E)]

== 14. Кратчайшие пути в графах
Нахождение кратчайших путей из одной вершины в невзвешенных графах, поиск в ширину.
Нахождение кратчайших путей из одной вершины в графах с положительными весами,
алгоритм Дейкстры,
оценка времени работы при различных реализациях очереди с приоритетами
(массивом, двоичной кучей, d-ичной кучей).

=== Кратчайшие пути в невзвешенном графе
* BFS (Breadth-First Search) -- поиск в ширину
* Как DFS, только вместо стека -- очередь
* Кратчайший путь -- простой

=== Алгоритм Дейкстры
* Поиск с приоритетной очередью
* Неотрицательные веса
* Кратчайший путь -- простой
* Похоже на алгоритм Прима, но вес вершины в очереди -- длина пути до неё, а не последнее ребро на этом пути
* Ломается при отрицательных весах

Алгоритм Дейкстры разбивает вершины на уже обработанные -- stem:[S],
находящиеся в очереди -- stem:[Q], и остальные -- stem:[R].
Расстояние до пройденных вершин корректно.
Доказательство:
База -- начальный шаг.
Расстояние до старта -- stem:[rho(A -> A) = 0].

Переход: обрабатываем вершину stem:[v].
[stem]
++++
forall u in S => hat rho(A -> u) = rho(A -> u)

forall u in Q => hat rho(A -> u) >= hat rho(A -> v)

forall u in R => hat rho(A -> u) = +oo

hat rho(A -> v)
= min_u { hat rho(A -> u) + w(u -> v) | u in S } = \
= min_u { rho(A -> u) + w(u -> v) | u in S }
++++

Любой путь stem:[A ->> v] будет проходить через какую-то вершину из stem:[Q],
но любой путь до них из stem:[S], не проходящий через stem:[Q],
будет иметь длину не меньше оценки расстояния до stem:[v].
Зайти в stem:[R], не проходя через stem:[Q], невозможно,
т.к. все вершины, в которые есть рёбра из stem:[S],
есть в stem:[S uu Q].
То есть такого пути, что stem:[rho(A -> v) < hat rho(A -> v)], не существует.

==== Время работы:

* Всегда -- stem:[O(V * "добавление" + V * "извлечение" + E * "обновление ключа")]
* На массиве -- stem:[O(V^2 + E) = O(V^2)]
* Приоритетная очередь на куче -- stem:[O(E log V)] (т.к. добавим и извлечём не больше вершин, чем рассмотрим рёбер)
* stem:[d]-ичная куча принципиально не отличается от двоичной:
  stem:[O(V * log_d V + V * d * log_d V + E * log_d V) = O(Vd log_d V + E log_d V)]

== 15. Кратчайшие пути в графах с отрицательными рёбрами
Алгоритм Беллмана-Форда, проверка наличия цикла отрицательного веса.
Кратчайшие пути в ациклических ориентированных графах.
Кратчайшие пути между всеми парами вершин: алгоритм Флойда-Уоршелла.

=== Алгоритм Беллмана-Форда
Релаксация на ребре может улучшить оценку, но не испортить её.

Пусть stem:[A -> v_1 -> ... -> v_k] -- кратчайший путь из stem:[A] в stem:[v_k].
Произведём последовательно релаксацию рёбер stem:[A -> v_1, v_1 -> v_2, ..., v_(k - 1) -> v_k].
Тогда stem:[forall i <= k => rho(A -> v_i) = hat rho(A -> v_i)].

Любой кратчайший путь -- простой, следовательно, содержит не более stem:[|V| - 1] рёбер.

Давайте выберем такую последовательность рёбер,
что любая последовательность длины не более stem:[|V| - 1]
будет её подпоследовательностью.
Можно заметить, что повторение произвольной последовательности
всех рёбер stem:[|V| - 1] раз будет подходить.

Тогда алгоритм Беллмана-Форда:

. Инициализировать расстояния:
  * stem:[hat rho(A -> v) := +oo]
  * stem:[hat rho(A -> A) := 0]
. Повторить stem:[|V| - 1] раз
  * Пройти по всем рёбрам
  ** Каждое ребро stem:[u -> v] прорелаксировать:
     stem:[hat rho(A -> v) := min {hat rho(A -> v); hat rho(A -> u) + w(u -> v)}]

Тогда у каждой вершины будет записано расстояние до stem:[A],
то есть длина кратчайшего пути, и последнее ребро этого пути.

Можно заметить, что если не произошло ни одной релаксации на очередной итерации,
то внешний цикл можно остановить, новых релаксаций уже не произойдёт.

Время работы: stem:[O(|V| * |E|)].

==== Проверка цикла отрицательного веса
Можно пройти по всем рёбрам в алгоритме Беллмана-Форда ещё раз.
Если произошла хотя бы одна релаксация, то она принадлежит отрицательному циклу.

=== Кратчайшие пути в ациклических ориентированных графах
. Топологическая сортировка, stem:[O(|V| + |E|)]
. Идём слева направо (от истоков к стокам)
  * Релаксируем каждое ребро, stem:[O(|E|)]

Время работы -- stem:[O(|V| + |E|)].

=== Алгоритм Флойда-Уоршелла
Динамическое программирование.
Пусть stem:[d(i -> j, k)] -- расстояние между
вершинами stem:[v_i] и stem:[v_j],
если промежуточными вершинами могут быть только stem:[v_t | t <= k].
Тогда
[stem]
++++
d(i -> j, 0) = w(v_i -> v_j)

d(i -> j, k + 1) = min {d(i -> j, k); d(i -> k, k) + d(k -> j, k)}
++++
Если происходит релаксация stem:[d(i -> j, k + 1) = d(i -> k, k) + d(k -> j, k)],
то последнее ребро на пути stem:[v_i -> v_j]
-- это последнее ребро на пути stem:[v_k -> j].
При этом достаточно хратить только один слой stem:[d],
поскольку в любой момент времени stem:[d(i -> j, k) >= rho(v_i -> v_j)].

Время работы -- stem:[O(|V|^3)], затраты памяти -- stem:[O(|V|^2)].

== Примечания

Билет состоит из двух вопросов. При подготовке билетов пользоваться
любыми источниками запрещается. Билеты рассказываются устно. Кроме
материала билета нужно уметь отвечать и на вопросы по другим билетам.
После ответа выдаётся задача. Перед получением билета студенту
предлагается написать тест. Оценка за тест -- это максимальная оценка,
которую студент может получить за экзамен.
